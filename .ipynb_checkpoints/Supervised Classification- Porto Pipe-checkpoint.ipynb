{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification with Titanic\n",
    "### AKA \"Going Wide\"\n",
    "Author: Nick Brooks\n",
    "\n",
    "Date: Summer 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic('matplotlib inline')\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import feature_selection\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "#Evalaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Grid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats as st\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Esemble Voting\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Performance\n",
    "%load_ext memory_profiler\n",
    "\n",
    "# Stacking\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#os.chdir(r\"C:\\Users\\Nicol\\Google Drive\\Learning\\Jupyter\\Classification and Regression\\Titanic\")\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Master Parameters:\n",
    "n_splits = 5\n",
    "n_iter = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 8) (891,) (418, 8)\n"
     ]
    }
   ],
   "source": [
    "#Titanic\n",
    "path = r\"C:\\Users\\Nicol\\Google Drive\\Learning\\Jupyter\\Classification and Regression\\Titanic\"\n",
    "#path = r\"/Users/nicapotato/Google Drive/Learning/Jupyter/Titanic\"\n",
    "\n",
    "#train_df = pd.read_csv(open(os.path.join(path, \"clean_train.csv\"), \"r\")) \n",
    "#test_df = pd.read_csv(open(os.path.join(path, \"clean_test.csv\"), \"r\"))\n",
    "\n",
    "#train_df = pd.read_csv(open(os.path.join(path, \"clean_train2.csv\"), \"r\")) \n",
    "#test_df = pd.read_csv(open(os.path.join(path, \"clean_test2.csv\"), \"r\")) \n",
    "\n",
    "train_df = pd.read_csv(open(os.path.join(path,\n",
    "                \"Data/clean_train_nick.csv\"), \"r\"), index_col=\"PassengerId\")\n",
    "test_df = pd.read_csv(open(os.path.join(path,\n",
    "                \"Data/clean_test_nick.csv\"), \"r\"), index_col=\"PassengerId\") \n",
    "\n",
    "X = train_df.drop([\"Survived\"] , axis=1)\n",
    "y = train_df[\"Survived\"]\n",
    "\n",
    "# use train/test split with different random_state values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "\n",
    "#test_df  = test_df.drop([\"PassengerId\"] , axis=1).copy()\n",
    "print(X.shape, y.shape, test_df.shape)\n",
    "\n",
    "results = pd.DataFrame(columns=['Model','Para','Test_Score','CV Mean','CV STDEV'])\n",
    "ensemble_models= {}\n",
    "\n",
    "def save(model, modelname):\n",
    "    global results\n",
    "    model.best_estimator_.fit(X, y)\n",
    "    submission = model.predict(test_df)\n",
    "    df = pd.DataFrame({'PassengerId':test_df.index, \n",
    "                           'Survived':submission})\n",
    "    df.to_csv((os.path.join(path,(\"submissions/{}.csv\".format(modelname)))),header=True,index=False)\n",
    "    \n",
    "    scores = cross_val_score(model.best_estimator_, X_train, y_train, cv=5, scoring='accuracy', verbose =0)\n",
    "    CV_scores = scores.mean()\n",
    "    STDev = scores.std()\n",
    "    Test_scores = model.score(X_test, y_test)\n",
    "    # print(metrics.accuracy_score(model.predict(X_test), y_test)) # Same\n",
    "\n",
    "    # CV and Save scoress\n",
    "    results = results.append({'Model': modelname,'Para': model.best_params_,'Test_Score': Test_scores,\n",
    "                             'CV Mean':CV_scores, 'CV STDEV': STDev}, ignore_index=True)\n",
    "    ensemble_models[modelname] = model.best_estimator_\n",
    "\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (CV_scores, STDev, modelname))\n",
    "    print(\"Optimal Model Parameters: {}\".format(grid.best_params_))\n",
    "    print('Test_Score:', Test_scores)\n",
    "\n",
    "    # with open((os.path.join(path,(r\"Pickle/{}.pickle\".format(modelname)))), 'wb') as f: pickle.dump(model.best_estimator_, f)\n",
    "        \n",
    "def norm_save(model,score, modelname):\n",
    "    global results\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(test_df)\n",
    "    df = pd.DataFrame({'PassengerId':test_df.index, \n",
    "                           'Survived':submission})\n",
    "    \n",
    "    CV_Score = score.mean()\n",
    "    Test_Score = model.score(X_test, y_test)\n",
    "    STDev = score.std()\n",
    "    \n",
    "    # CV and Save Scores\n",
    "    Test_Score = model.score(X_test, y_test)\n",
    "    results = results.append({'Model': modelname,'Para': model,'Test_Score': Test_Score,\n",
    "                             'CV Mean':CV_Score, 'CV STDEV': STDev}, ignore_index=True)\n",
    "    ensemble_models[modelname] = model\n",
    "    df.to_csv((os.path.join(path,(\"submissions/{}.csv\".format(modelname)))),header=True,index=False)\n",
    "    \n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (CV_Score, STDev, modelname))  \n",
    "    print('Test_Score:', Test_Score)\n",
    "    \n",
    "    # with open((os.path.join(path,(r\"Pickle/{}.pickle\".format(modelname)))), 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.616162\n",
      "1    0.383838\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts(normalize=True))\n",
    "# Should Balance This DataSet through resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stratified Cross Validation\n",
    "cv = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 1 to 891\n",
      "Data columns (total 8 columns):\n",
      "Pclass      891 non-null int64\n",
      "Sex         891 non-null int64\n",
      "Age         891 non-null float64\n",
      "SibSp       891 non-null int64\n",
      "Parch       891 non-null int64\n",
      "Fare        891 non-null float64\n",
      "Embarked    891 non-null int64\n",
      "Title       891 non-null int64\n",
      "dtypes: float64(2), int64(6)\n",
      "memory usage: 102.6 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 418 entries, 892 to 1309\n",
      "Data columns (total 8 columns):\n",
      "Pclass      418 non-null int64\n",
      "Sex         418 non-null int64\n",
      "Age         418 non-null float64\n",
      "SibSp       418 non-null int64\n",
      "Parch       418 non-null int64\n",
      "Fare        418 non-null float64\n",
      "Embarked    418 non-null int64\n",
      "Title       418 non-null int64\n",
      "dtypes: float64(2), int64(6)\n",
      "memory usage: 29.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X.info())\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Classification\n",
    "Probabilistically determine the label from the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.77 (+/- 0.03) [Gaussian]\n",
      "Test_Score: 0.821229050279\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "score = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "norm_save(model,score, \"Gaussian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.78 (+/- 0.02) [Logistic_Regression]\n",
      "Test_Score: 0.815642458101\n"
     ]
    }
   ],
   "source": [
    "model= LogisticRegression()\n",
    "score = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "norm_save(LogisticRegression(),score, \"Logistic_Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epsilon', 'random_state', 'momentum', 'beta_1', 'beta_2', 'hidden_layer_sizes', 'early_stopping', 'learning_rate', 'tol', 'solver', 'activation', 'alpha', 'warm_start', 'learning_rate_init', 'max_iter', 'verbose', 'power_t', 'shuffle', 'nesterovs_momentum', 'validation_fraction', 'batch_size'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLPClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Train CV Accuracy: 0.65 (+/- 0.03) [RSNeural_Net]\n",
      "Optimal Model Parameters: {'hidden_layer_sizes': 464, 'early_stopping': True, 'learning_rate': 'adaptive', 'activation': 'identity', 'alpha': 100, 'max_iter': 4641}\n",
      "Test_Score: 0.625698324022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:   10.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Start with a RandomSearchCV to efficiently Narrow the Ballpark\n",
    "param_grid ={'max_iter': np.logspace(1, 5, 10).astype(\"int32\"),\n",
    "             'hidden_layer_sizes': np.logspace(2, 3, 4).astype(\"int32\"),\n",
    "             'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
    "             'learning_rate': ['adaptive'],\n",
    "             'early_stopping': [True],\n",
    "             'alpha': np.logspace(2, 3, 4).astype(\"int32\")\n",
    "            }\n",
    "\n",
    "model = MLPClassifier()\n",
    "\n",
    "grid = RandomizedSearchCV(model,\n",
    "                    param_grid, cv=cv, scoring='accuracy',\n",
    "                    verbose=1, n_iter=n_iter)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid, \"RSNeural_Net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esemble Method\n",
    "\n",
    "Means that a bunch of the model get created and are aggregated at the end for best performance.\n",
    "\n",
    "## Bagging, Bootstrap\n",
    "\n",
    "Aka Bootstrap- creates a bunch of trees using a random 3/4 the the data for each, while using sampling without replacement, which means that values may be sampled multiple times.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "\n",
    "HyperParameters:\n",
    "- max_features: This is the random subset of features to be used for splitting node, the lower the better to reduce variance. For Classification model, ideal max_features = sqr(n_var)\n",
    "- n_estimators: # of trees built before average prediciton is made\n",
    "- min_sample_leaf: End node of trees. Too small = more noise. For Regression tree.\n",
    "- n_jobs: computer processors utilized. -1 = no restrictions\n",
    "- random_state: seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.5002837362\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=300, max_samples=0.8,\n",
    "                        random_state=1)\n",
    "\n",
    "print(cross_val_score(bag, X, y, cv=10, scoring='accuracy').mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 20,  45,  70,  95, 120, 145, 170, 195, 220, 245, 270, 295, 320,\n",
       "       345, 370, 395, 420, 445, 470, 495])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(20, 500, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.81 (+/- 0.02) [Bagger_ensemble]\n",
      "Optimal Model Parameters: {'n_estimators': 226}\n",
      "Test_Score: 0.938547486034\n"
     ]
    }
   ],
   "source": [
    "param_grid ={'n_estimators': st.randint(20, 500)}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "#bag = BaggingClassifier(tree)\n",
    "\n",
    "grid = RandomizedSearchCV(BaggingClassifier(tree),\n",
    "                    param_grid, cv=cv, scoring='accuracy',\n",
    "                    verbose=1,n_iter=n_iter)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid, \"Bagger_ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Trees are created wih a randomly picked subset of observations and variables. More uncorrelated splits, less overemphasis on certain features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854748603352\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_jobs', 'n_estimators', 'random_state', 'class_weight', 'min_impurity_split', 'verbose', 'min_samples_leaf', 'min_weight_fraction_leaf', 'warm_start', 'max_depth', 'oob_score', 'criterion', 'min_impurity_decrease', 'bootstrap', 'min_samples_split', 'max_features', 'max_leaf_nodes'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.81 (+/- 0.04) [Random_Forest]\n",
      "Optimal Model Parameters: {'max_depth': 9, 'max_features': 0.65000000000000013, 'max_leaf_nodes': 7, 'n_estimators': 300}\n",
      "Test_Score: 0.837988826816\n"
     ]
    }
   ],
   "source": [
    "param_grid ={'max_depth': st.randint(6, 11),\n",
    "             'n_estimators':st.randint(300, 500),\n",
    "             'max_features':np.arange(0.5,.81, 0.05),\n",
    "            'max_leaf_nodes':st.randint(6, 10)}\n",
    "#param_grid ={'n_estimators':[200]}\n",
    "\n",
    "#model = feature_selection.RFE(RandomForestClassifier())\n",
    "model= RandomForestClassifier()\n",
    "\n",
    "grid = RandomizedSearchCV(model,\n",
    "                    param_grid, cv=cv,\n",
    "                    scoring='accuracy',\n",
    "                    verbose=1,n_iter=n_iter)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid, \"Random_Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremely Randomized Trees (ExtraTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier: Boosting Method\n",
    "\n",
    "Method, similarly to deep learning, applies weights to all data points and optimizes them using the loss function. Fixes mistakes by assigning high weights to them during iterative process.\n",
    "\n",
    "Iterates through multiple models in order to determine the best boundaries. It relies on using weak models to determine the pattern, and evantually creates a strong combination of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.81 (+/- 0.01) [AdaBoost_Ensemble]\n",
      "Optimal Model Parameters: {'learning_rate': 1.6000000000000001, 'n_estimators': 81}\n",
      "Test_Score: 0.837988826816\n"
     ]
    }
   ],
   "source": [
    "param_grid ={'n_estimators':st.randint(50, 400),\n",
    "            'learning_rate':np.arange(.1, 4, .5)}\n",
    "\n",
    "grid = RandomizedSearchCV(AdaBoostClassifier(),\n",
    "                    param_grid,cv=cv, scoring='accuracy',\n",
    "                    verbose=1, n_iter=n_iter)\n",
    "\n",
    "grid.fit(X, y);\n",
    "save(grid, \"AdaBoost_Ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier\n",
    "\n",
    "Part of the Generalized Boosting Algorithm family.\n",
    "\n",
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions.\n",
    "\n",
    "Part of the generalized boosting algorithms. Can use more loss functions than AdaBoost, and uses gradients instead of high-weight data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#?GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:   42.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.81 (+/- 0.03) [Gradient_Boosting]\n",
      "Optimal Model Parameters: {'learning_rate': 0.16000000000000003, 'max_depth': 3.5, 'n_estimators': 237, 'loss': 'deviance'}\n",
      "Test_Score: 0.921787709497\n"
     ]
    }
   ],
   "source": [
    "param_grid ={'n_estimators':st.randint(100, 400),\n",
    "            'loss': ['deviance', 'exponential'],\n",
    "            'learning_rate':np.arange(0.01, 0.32,.05),\n",
    "            'max_depth': np.arange(2, 4.1, .5)}\n",
    "\n",
    "grid = RandomizedSearchCV(GradientBoostingClassifier(),\n",
    "                    param_grid,cv=cv,\n",
    "                    scoring='accuracy',\n",
    "                    verbose=1, n_iter=n_iter)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid, \"Gradient_Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB - eXtreme Gradient Boosting\n",
    "\n",
    "Optimized Generalized Gradient Booster, developped in 2014, competetes will in Kaggle Competitions!\n",
    "\n",
    "Install: https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "# os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "# import xgboost as xgb\n",
    "# from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# param_grid = {'max_depth': st.randint(1, 5),  # the maximum depth of each tree\n",
    "#          'objective': 'binary:logistic'}\n",
    "\n",
    "# model = XGBClassifier(n_estimators = num_rounds,\n",
    "#                         objective= 'binary:logistic')\n",
    "\n",
    "# grid = RandomizedSearchCV(model,\n",
    "#                     param_grid,cv=cv,\n",
    "#                     scoring='accuracy',\n",
    "#                     verbose=1, n_iter=n_iter)\n",
    "# grid.fit(X_train,y_train, early_stopping_rounds=20, eval_set=[(X_test,\n",
    "# y_test)], verbose=False)\n",
    "# #save(grid, \"Gradient_Boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['objective', 'n_jobs', 'colsample_bylevel', 'seed', 'colsample_bytree', 'random_state', 'learning_rate', 'n_estimators', 'missing', 'scale_pos_weight', 'min_child_weight', 'max_depth', 'reg_lambda', 'booster', 'max_delta_step', 'silent', 'subsample', 'reg_alpha', 'gamma', 'base_score', 'nthread'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.  ,  0.  ,  0.25,  0.5 ,  0.75,  1.  ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.randint(3, 100)\n",
    "st.uniform.cdf([0, 1, 2, 3, 4, 5], loc=1, scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Train CV Accuracy: 0.79 (+/- 0.04) [Sci_kit XGB]\n",
      "Optimal Model Parameters: {'max_depth': 28, 'subsample': 0.90555175643033681, 'colsample_bytree': 0.98352497008302076, 'learning_rate': 0.22091698571780871, 'n_estimators': 28, 'reg_alpha': 4.4269488668367458, 'gamma': 1.7821128500742378, 'min_child_weight': 24.30807238193173}\n",
      "Test_Score: 0.832402234637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "num_rounds = 100\n",
    "one_to_left = st.beta(10, 1)  \n",
    "from_zero_positive = st.expon(0, 50)\n",
    "\n",
    "params = {  \n",
    "    \"n_estimators\": st.randint(3, 100),\n",
    "    \"max_depth\": st.randint(3, 40),\n",
    "    \"learning_rate\": st.uniform(0.05, 0.4),\n",
    "    \"colsample_bytree\": one_to_left,\n",
    "    \"subsample\": one_to_left,\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "    'reg_alpha': from_zero_positive,\n",
    "    \"min_child_weight\": from_zero_positive,\n",
    "}\n",
    "\n",
    "xgbreg = XGBClassifier(n_estimators = num_rounds,\n",
    "                        objective= 'binary:logistic',\n",
    "                       nthreads=-1)\n",
    "\n",
    "grid = RandomizedSearchCV(xgbreg, params, n_jobs=1, verbose=1)  \n",
    "grid.fit(X_train,y_train, verbose=False)\n",
    "save(grid, \"Sci_kit XGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xgBoost - CV Train : 0.80\n",
      "xgBoost - Train : 0.86\n",
      "xgBoost - Test : 0.84\n",
      "Train CV Accuracy: 0.80 (+/- 0.03) [XGBsklearn]\n",
      "Test_Score: 0.860335195531\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(n_estimators = num_rounds,\n",
    "                        objective= 'binary:logistic')\n",
    "\n",
    "# use early_stopping_rounds to stop the cv when there is no score imporovement\n",
    "model.fit(X_train,y_train, early_stopping_rounds=20, eval_set=[(X_test,\n",
    "y_test)], verbose=False)\n",
    "score = cross_val_score(model, X_train,y_train, cv=cv)\n",
    "print(\"\\nxgBoost - CV Train : %.2f\" % score.mean())\n",
    "print(\"xgBoost - Train : %.2f\" % metrics.accuracy_score(model.predict(X_train), y_train))\n",
    "print(\"xgBoost - Test : %.2f\" % metrics.accuracy_score(model.predict(X_test), y_test))\n",
    "\n",
    "norm_save(model,score, \"XGBsklearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of trees/estimators is 6\n",
      "[0]\ttest-error:0.201117\ttrain-error:0.16573\n",
      "[1]\ttest-error:0.206704\ttrain-error:0.164326\n",
      "[2]\ttest-error:0.206704\ttrain-error:0.164326\n",
      "[3]\ttest-error:0.206704\ttrain-error:0.164326\n",
      "[4]\ttest-error:0.167598\ttrain-error:0.157303\n",
      "[5]\ttest-error:0.167598\ttrain-error:0.160112\n",
      "XGB - Train : 0.83\n",
      "XGB - Test : 0.84\n",
      "Train CV Accuracy: 0.83 (+/- 0.00) [XGBstandard]\n",
      "Test_Score: 0.860335195531\n"
     ]
    }
   ],
   "source": [
    "num_rounds = 100\n",
    "xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "xgtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# set xgboost params\n",
    "param = {'max_depth': 3,  # the maximum depth of each tree\n",
    "         'objective': 'binary:logistic'}\n",
    "\n",
    "clf_xgb_cv = xgb.cv(param, xgtrain, num_rounds, \n",
    "                    stratified=True, \n",
    "                    nfold=n_splits, \n",
    "                    early_stopping_rounds=20)\n",
    "print(\"Optimal number of trees/estimators is %i\" % clf_xgb_cv.shape[0])\n",
    "\n",
    "watchlist  = [(xgtest,'test'), (xgtrain,'train')]                \n",
    "clf_xgb = xgb.train(param, xgtrain,clf_xgb_cv.shape[0], watchlist)\n",
    "\n",
    "# predict function will produce the probability \n",
    "# so we'll use 0.5 cutoff to convert probability to class label\n",
    "y_train_pred = (clf_xgb.predict(xgtrain, ntree_limit=clf_xgb.best_iteration) > 0.5).astype(int)\n",
    "y_test_pred = (clf_xgb.predict(xgtest, ntree_limit=clf_xgb.best_iteration) > 0.5).astype(int)\n",
    "score= metrics.accuracy_score(y_test_pred, y_test)\n",
    "print(\"XGB - Train : %.2f\" % score)\n",
    "print(\"XGB - Test : %.2f\" % metrics.accuracy_score(y_train_pred, y_train))\n",
    "norm_save(model,score, \"XGBstandard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Train CV Accuracy: 0.81 (+/- 0.03) [KNN]\n",
      "Optimal Model Parameters: {'n_neighbors': 6, 'weights': 'uniform'}\n",
      "Test_Score: 0.854748603352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    3.3s finished\n"
     ]
    }
   ],
   "source": [
    "param_grid ={'n_neighbors': st.randint(1,40),\n",
    "            'weights':['uniform','distance']\n",
    "            }\n",
    "\n",
    "grid = RandomizedSearchCV(KNeighborsClassifier(),\n",
    "                    param_grid,cv=cv, scoring='accuracy',\n",
    "                    verbose=1, n_iter=n_iter)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "save(grid, \"KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative Classification\n",
    "Model new points by seeing where it falls upon a divide.\n",
    "Fast prediction phase, work well in high dimensional data, versatile\n",
    "\n",
    "Costly at high quantities of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epsilon', 'n_jobs', 'eta0', 'fit_intercept', 'warm_start', 'n_iter', 'random_state', 'learning_rate', 'verbose', 'alpha', 'average', 'max_iter', 'class_weight', 'power_t', 'penalty', 'shuffle', 'l1_ratio', 'tol', 'loss'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGDClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Train CV Accuracy: 0.74 (+/- 0.07) [StochasticGradientDescent]\n",
      "Optimal Model Parameters: {'loss': 'squared_hinge'}\n",
      "Test_Score: 0.72625698324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "param_grid ={'loss':[\"hinge\",\"log\",\"modified_huber\",\"squared_hinge\",\"epsilon_insensitive\",\"squared_epsilon_insensitive\"]\n",
    "            }\n",
    "\n",
    "grid = GridSearchCV(SGDClassifier(),\n",
    "                    param_grid,cv=cv, scoring='accuracy',\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid, \"StochasticGradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "Creates a linear divide between point to classify. Maximizes the distance of the discriminatory margin.\n",
    "\n",
    "Hyperparameters:\n",
    "- C: Hardness of the margin. Higher C, less softening.\n",
    "\n",
    "\n",
    "Radial Basis Function (RBF)\n",
    "- Gamma: how far the influence of a single training example raches. low=far, high=close, Inverse of the radius of influence of samples selected by the model as support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dual', 'random_state', 'class_weight', 'verbose', 'max_iter', 'intercept_scaling', 'penalty', 'C', 'fit_intercept', 'tol', 'loss', 'multi_class'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearSVC().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.80 (+/- 0.03) [LinearSV]\n",
      "Test_Score: 0.815642458101\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "model = LinearSVC()\n",
    "#Fit Model\n",
    "scores= cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "norm_save(model, scores, \"LinearSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function Kernel - SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['probability', 'degree', 'random_state', 'class_weight', 'verbose', 'gamma', 'max_iter', 'decision_function_shape', 'cache_size', 'shrinking', 'C', 'coef0', 'tol', 'kernel'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.82 (+/- 0.02) [SVCrbf]\n",
      "Optimal Model Parameters: {'svc__C': 5364, 'svc__gamma': 0.0027825594022071257}\n",
      "Test_Score: 0.837988826816\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel= 'rbf', probability=True)\n",
    "\n",
    "model = Pipeline(steps=[('svc', svc)])\n",
    "\n",
    "\n",
    "param_grid = {'svc__C': st.randint(1,10000),\n",
    "              'svc__gamma': np.logspace(1, -7, 10)}\n",
    "\n",
    "grid = RandomizedSearchCV(model, param_grid,\n",
    "                          cv=cv, verbose=1, scoring='accuracy',\n",
    "                         n_iter=n_iter)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid, \"SVCrbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# param_grid = {'C':st.randint(1,40),'kernel':['linear'], \"probability\" : [True]}\n",
    "\n",
    "# model = SVC()\n",
    "# grid = RandomizedSearchCV(model,\n",
    "#                     param_grid, cv=cv,\n",
    "#                     scoring='accuracy', verbose=1)\n",
    "\n",
    "# grid.fit(X, y)\n",
    "# save(grid, \"SVCLinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: Principle Components Analysis and Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.82 (+/- 0.02) [PCA_SVC]\n",
      "Optimal Model Parameters: {'pca__n_components': 6, 'svc__C': 4902, 'svc__gamma': 0.0027825594022071257}\n",
      "Test_Score: 0.832402234637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pca = PCA()\n",
    "svc = SVC(kernel= 'rbf',probability=True)\n",
    "\n",
    "model = Pipeline(steps=[('pca',pca),\n",
    "                        ('svc', svc)])\n",
    "\n",
    "\n",
    "param_grid = {'svc__C': st.randint(1,10000),\n",
    "              'svc__gamma': np.logspace(1, -7, 10),\n",
    "             'pca__n_components': st.randint(1,len(X.columns))}\n",
    "\n",
    "grid = RandomizedSearchCV(model, param_grid,\n",
    "                          cv=cv, verbose=1,\n",
    "                         n_iter=n_iter)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid, \"PCA_SVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.to_csv(\"Titanic/results.csv\",index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.read_csv(open(os.path.join(path, \"results.csv\"), \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Para</th>\n",
       "      <th>Test_Score</th>\n",
       "      <th>CV Mean</th>\n",
       "      <th>CV STDEV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bagger_ensemble</td>\n",
       "      <td>{'n_estimators': 226}</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.813237</td>\n",
       "      <td>0.021745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient_Boosting</td>\n",
       "      <td>{'learning_rate': 0.16000000000000003, 'max_de...</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.809022</td>\n",
       "      <td>0.034348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBsklearn</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.025407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBstandard</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>KNN</td>\n",
       "      <td>{'n_neighbors': 6, 'weights': 'uniform'}</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.811859</td>\n",
       "      <td>0.032075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>{'max_depth': 9, 'max_features': 0.65000000000...</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.813208</td>\n",
       "      <td>0.039981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoost_Ensemble</td>\n",
       "      <td>{'learning_rate': 1.6000000000000001, 'n_estim...</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.810401</td>\n",
       "      <td>0.012438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SVCrbf</td>\n",
       "      <td>{'svc__C': 5364, 'svc__gamma': 0.0027825594022...</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.818832</td>\n",
       "      <td>0.022658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sci_kit XGB</td>\n",
       "      <td>{'max_depth': 28, 'subsample': 0.9055517564303...</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.786506</td>\n",
       "      <td>0.044517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PCA_SVC</td>\n",
       "      <td>{'pca__n_components': 6, 'svc__C': 4902, 'svc_...</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.821639</td>\n",
       "      <td>0.017969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gaussian</td>\n",
       "      <td>GaussianNB(priors=None)</td>\n",
       "      <td>0.821229</td>\n",
       "      <td>0.773184</td>\n",
       "      <td>0.025625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.776536</td>\n",
       "      <td>0.016945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LinearSV</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.803352</td>\n",
       "      <td>0.033594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>StochasticGradientDescent</td>\n",
       "      <td>{'loss': 'squared_hinge'}</td>\n",
       "      <td>0.726257</td>\n",
       "      <td>0.738737</td>\n",
       "      <td>0.067388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RSNeural_Net</td>\n",
       "      <td>{'hidden_layer_sizes': 464, 'early_stopping': ...</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.648892</td>\n",
       "      <td>0.027143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  \\\n",
       "3             Bagger_ensemble   \n",
       "6           Gradient_Boosting   \n",
       "8                  XGBsklearn   \n",
       "9                 XGBstandard   \n",
       "10                        KNN   \n",
       "4               Random_Forest   \n",
       "5           AdaBoost_Ensemble   \n",
       "13                     SVCrbf   \n",
       "7                 Sci_kit XGB   \n",
       "14                    PCA_SVC   \n",
       "0                    Gaussian   \n",
       "1         Logistic_Regression   \n",
       "12                   LinearSV   \n",
       "11  StochasticGradientDescent   \n",
       "2                RSNeural_Net   \n",
       "\n",
       "                                                 Para  Test_Score   CV Mean  \\\n",
       "3                               {'n_estimators': 226}    0.938547  0.813237   \n",
       "6   {'learning_rate': 0.16000000000000003, 'max_de...    0.921788  0.809022   \n",
       "8   XGBClassifier(base_score=0.5, booster='gbtree'...    0.860335  0.797203   \n",
       "9   XGBClassifier(base_score=0.5, booster='gbtree'...    0.860335  0.832402   \n",
       "10           {'n_neighbors': 6, 'weights': 'uniform'}    0.854749  0.811859   \n",
       "4   {'max_depth': 9, 'max_features': 0.65000000000...    0.837989  0.813208   \n",
       "5   {'learning_rate': 1.6000000000000001, 'n_estim...    0.837989  0.810401   \n",
       "13  {'svc__C': 5364, 'svc__gamma': 0.0027825594022...    0.837989  0.818832   \n",
       "7   {'max_depth': 28, 'subsample': 0.9055517564303...    0.832402  0.786506   \n",
       "14  {'pca__n_components': 6, 'svc__C': 4902, 'svc_...    0.832402  0.821639   \n",
       "0                             GaussianNB(priors=None)    0.821229  0.773184   \n",
       "1   LogisticRegression(C=1.0, class_weight=None, d...    0.815642  0.776536   \n",
       "12  LinearSVC(C=1.0, class_weight=None, dual=True,...    0.815642  0.803352   \n",
       "11                          {'loss': 'squared_hinge'}    0.726257  0.738737   \n",
       "2   {'hidden_layer_sizes': 464, 'early_stopping': ...    0.625698  0.648892   \n",
       "\n",
       "    CV STDEV  \n",
       "3   0.021745  \n",
       "6   0.034348  \n",
       "8   0.025407  \n",
       "9   0.000000  \n",
       "10  0.032075  \n",
       "4   0.039981  \n",
       "5   0.012438  \n",
       "13  0.022658  \n",
       "7   0.044517  \n",
       "14  0.017969  \n",
       "0   0.025625  \n",
       "1   0.016945  \n",
       "12  0.033594  \n",
       "11  0.067388  \n",
       "2   0.027143  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = results.sort_values(by=[\"Test_Score\"], ascending=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting\n",
    "\n",
    "Hard- Mode\n",
    "Soft- Probabilistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dic = {}\n",
    "# for name in results[\"Model\"]:\n",
    "#     open_file = open(os.path.join(path,\"Pickle/{}.pickle\".format(name)), \"rb\")\n",
    "#     dic[name] = pickle.load(open_file)\n",
    "#     open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.82 (+/- 0.02) [PCA_SVC]\n",
      "Test Accuracy: 0.83 \n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [Bagger_ensemble]\n",
      "Test Accuracy: 0.94 \n",
      "Train CV Accuracy: 0.82 (+/- 0.04) [Random_Forest]\n",
      "Test Accuracy: 0.84 \n",
      "Train CV Accuracy: 0.81 (+/- 0.03) [Gradient_Boosting]\n",
      "Test Accuracy: 0.92 \n",
      "Train CV Accuracy: 0.81 (+/- 0.03) [XGBsklearn]\n",
      "Test Accuracy: 0.86 \n",
      "Train CV Accuracy: 0.81 (+/- 0.01) [AdaBoost_Ensemble]\n",
      "Test Accuracy: 0.84 \n",
      "Train CV Accuracy: 0.65 (+/- 0.03) [RSNeural_Net]\n",
      "Test Accuracy: 0.73 \n",
      "Train CV Accuracy: 0.77 (+/- 0.03) [LinearSV]\n",
      "Test Accuracy: 0.82 \n",
      "Train CV Accuracy: 0.79 (+/- 0.04) [Sci_kit XGB]\n",
      "Test Accuracy: 0.83 \n",
      "Train CV Accuracy: 0.81 (+/- 0.03) [XGBstandard]\n",
      "Test Accuracy: 0.86 \n",
      "Train CV Accuracy: 0.81 (+/- 0.03) [KNN]\n",
      "Test Accuracy: 0.85 \n",
      "Train CV Accuracy: 0.78 (+/- 0.03) [Logistic_Regression]\n",
      "Test Accuracy: 0.82 \n",
      "Train CV Accuracy: 0.74 (+/- 0.05) [StochasticGradientDescent]\n",
      "Test Accuracy: 0.75 \n",
      "Train CV Accuracy: 0.76 (+/- 0.04) [Gaussian]\n",
      "Test Accuracy: 0.82 \n",
      "Train CV Accuracy: 0.82 (+/- 0.02) [SVCrbf]\n",
      "Test Accuracy: 0.84 \n"
     ]
    }
   ],
   "source": [
    "models = list(zip(ensemble_models.values(), ensemble_models.keys()))\n",
    "# results[[\"Best_estimator\",\"Model\"]].apply(tuple, axis=1)\n",
    "# zip(results[\"Best_estimator\"],results[\"Model\"]) \n",
    "\n",
    "clfs = []\n",
    "print('5-fold cross validation:\\n')\n",
    "for clf, label in models:\n",
    "    scores = cross_val_score(clf, X_train, y_train,cv=5, scoring='accuracy', verbose=0)\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X, y)    \n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensembling(model, modelname):\n",
    "    global results\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(test_df)\n",
    "    df = pd.DataFrame({'PassengerId':test_df.index, \n",
    "                           'Survived':submission})\n",
    "    results = results.append({'Model': modelname,'Para': model, 'CV Mean': None,\n",
    "            'Test_Score':metrics.accuracy_score(clf.predict(X_test), y_test)}, ignore_index=True)\n",
    "    ensemble_models[modelname] = model\n",
    "    print(len(df))\n",
    "    df.to_csv((os.path.join(path,(r\"submissions/{}.csv\".format(modelname)))),header=True,index=False)\n",
    "    # with open((os.path.join(path,(r\"Pickle/{}.pickle\".format(modelname)))), 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Add all models above X accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Para</th>\n",
       "      <th>Test_Score</th>\n",
       "      <th>CV Mean</th>\n",
       "      <th>CV STDEV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bagger_ensemble</td>\n",
       "      <td>{'n_estimators': 226}</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.813237</td>\n",
       "      <td>0.021745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient_Boosting</td>\n",
       "      <td>{'learning_rate': 0.16000000000000003, 'max_de...</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.809022</td>\n",
       "      <td>0.034348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBsklearn</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.025407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBstandard</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>KNN</td>\n",
       "      <td>{'n_neighbors': 6, 'weights': 'uniform'}</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.811859</td>\n",
       "      <td>0.032075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>{'max_depth': 9, 'max_features': 0.65000000000...</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.813208</td>\n",
       "      <td>0.039981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoost_Ensemble</td>\n",
       "      <td>{'learning_rate': 1.6000000000000001, 'n_estim...</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.810401</td>\n",
       "      <td>0.012438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SVCrbf</td>\n",
       "      <td>{'svc__C': 5364, 'svc__gamma': 0.0027825594022...</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.818832</td>\n",
       "      <td>0.022658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sci_kit XGB</td>\n",
       "      <td>{'max_depth': 28, 'subsample': 0.9055517564303...</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.786506</td>\n",
       "      <td>0.044517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PCA_SVC</td>\n",
       "      <td>{'pca__n_components': 6, 'svc__C': 4902, 'svc_...</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.821639</td>\n",
       "      <td>0.017969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gaussian</td>\n",
       "      <td>GaussianNB(priors=None)</td>\n",
       "      <td>0.821229</td>\n",
       "      <td>0.773184</td>\n",
       "      <td>0.025625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.776536</td>\n",
       "      <td>0.016945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LinearSV</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.803352</td>\n",
       "      <td>0.033594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>StochasticGradientDescent</td>\n",
       "      <td>{'loss': 'squared_hinge'}</td>\n",
       "      <td>0.726257</td>\n",
       "      <td>0.738737</td>\n",
       "      <td>0.067388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RSNeural_Net</td>\n",
       "      <td>{'hidden_layer_sizes': 464, 'early_stopping': ...</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.648892</td>\n",
       "      <td>0.027143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  \\\n",
       "3             Bagger_ensemble   \n",
       "6           Gradient_Boosting   \n",
       "8                  XGBsklearn   \n",
       "9                 XGBstandard   \n",
       "10                        KNN   \n",
       "4               Random_Forest   \n",
       "5           AdaBoost_Ensemble   \n",
       "13                     SVCrbf   \n",
       "7                 Sci_kit XGB   \n",
       "14                    PCA_SVC   \n",
       "0                    Gaussian   \n",
       "1         Logistic_Regression   \n",
       "12                   LinearSV   \n",
       "11  StochasticGradientDescent   \n",
       "2                RSNeural_Net   \n",
       "\n",
       "                                                 Para  Test_Score   CV Mean  \\\n",
       "3                               {'n_estimators': 226}    0.938547  0.813237   \n",
       "6   {'learning_rate': 0.16000000000000003, 'max_de...    0.921788  0.809022   \n",
       "8   XGBClassifier(base_score=0.5, booster='gbtree'...    0.860335  0.797203   \n",
       "9   XGBClassifier(base_score=0.5, booster='gbtree'...    0.860335  0.832402   \n",
       "10           {'n_neighbors': 6, 'weights': 'uniform'}    0.854749  0.811859   \n",
       "4   {'max_depth': 9, 'max_features': 0.65000000000...    0.837989  0.813208   \n",
       "5   {'learning_rate': 1.6000000000000001, 'n_estim...    0.837989  0.810401   \n",
       "13  {'svc__C': 5364, 'svc__gamma': 0.0027825594022...    0.837989  0.818832   \n",
       "7   {'max_depth': 28, 'subsample': 0.9055517564303...    0.832402  0.786506   \n",
       "14  {'pca__n_components': 6, 'svc__C': 4902, 'svc_...    0.832402  0.821639   \n",
       "0                             GaussianNB(priors=None)    0.821229  0.773184   \n",
       "1   LogisticRegression(C=1.0, class_weight=None, d...    0.815642  0.776536   \n",
       "12  LinearSVC(C=1.0, class_weight=None, dual=True,...    0.815642  0.803352   \n",
       "11                          {'loss': 'squared_hinge'}    0.726257  0.738737   \n",
       "2   {'hidden_layer_sizes': 464, 'early_stopping': ...    0.625698  0.648892   \n",
       "\n",
       "    CV STDEV  \n",
       "3   0.021745  \n",
       "6   0.034348  \n",
       "8   0.025407  \n",
       "9   0.000000  \n",
       "10  0.032075  \n",
       "4   0.039981  \n",
       "5   0.012438  \n",
       "13  0.022658  \n",
       "7   0.044517  \n",
       "14  0.017969  \n",
       "0   0.025625  \n",
       "1   0.016945  \n",
       "12  0.033594  \n",
       "11  0.067388  \n",
       "2   0.027143  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_models = results[results.Model != 'LinearSV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = results.sort_values(by=[\"Test_Score\"], ascending=False)\n",
    "\n",
    "# keys = results.Model[:10]\n",
    "# allmodel= [ensemble_models.get(key) for key in results.Model[:10]]\n",
    "\n",
    "\n",
    "# soft= [ensemble_models.get(key) for key in results.Model[:7]]\n",
    "\n",
    "\n",
    "# keys =['RSNeural_Net', 'Gradient_Boosting', 'SVCLinear', 'StochasticGradientDescent', 'SVCrbf',\n",
    "#                       #'LinearSV',\n",
    "#                       'AdaBoost_Ensemble', 'Random_Forest', 'XGBstandard',\n",
    "#                       #'PCA_SVC',\n",
    "#        'XGBsklearn', 'Bagger_ensemble', 'Gaussian',\n",
    "#                       'Logistic_Regression', 'KNN']\n",
    "\n",
    "# keys = results.Model[:10]\n",
    "\n",
    "# bestkeys =['Gradient_Boosting', 'SVCLinear','XGBstandard',\n",
    "#        'XGBsklearn', 'Bagger_ensemble']\n",
    "\n",
    "# bestkeys = results.Model[:10]\n",
    "# for x in [\"\"]\n",
    "# del ensemble_models[PCA_SVC]\n",
    "\n",
    "# soft= [ensemble_models.get(key) for key in bestkeys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2-Voting Models: 5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [2-VM-Ensemble Soft Voting]\n",
      "Test Accuracy: 0.93 \n",
      "Train CV Accuracy: 0.81 (+/- 0.03) [2-VM-Ensemble Hard Voting]\n",
      "Test Accuracy: 0.93 \n",
      "\n",
      "\n",
      "3-Voting Models: 5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [3-VM-Ensemble Soft Voting]\n",
      "Test Accuracy: 0.92 \n",
      "Train CV Accuracy: 0.81 (+/- 0.03) [3-VM-Ensemble Hard Voting]\n",
      "Test Accuracy: 0.92 \n",
      "\n",
      "\n",
      "5-Voting Models: 5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.82 (+/- 0.02) [5-VM-Ensemble Soft Voting]\n",
      "Test Accuracy: 0.88 \n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [5-VM-Ensemble Hard Voting]\n",
      "Test Accuracy: 0.89 \n",
      "\n",
      "\n",
      "7-Voting Models: 5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.82 (+/- 0.02) [7-VM-Ensemble Soft Voting]\n",
      "Test Accuracy: 0.88 \n",
      "Train CV Accuracy: 0.82 (+/- 0.02) [7-VM-Ensemble Hard Voting]\n",
      "Test Accuracy: 0.86 \n",
      "\n",
      "\n",
      "10-Voting Models: 5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [10-VM-Ensemble Soft Voting]\n",
      "Test Accuracy: 0.87 \n",
      "Train CV Accuracy: 0.82 (+/- 0.03) [10-VM-Ensemble Hard Voting]\n",
      "Test Accuracy: 0.85 \n"
     ]
    }
   ],
   "source": [
    "# ### Ensemble Voting\n",
    "# w = [0,1,2,1,1,1,1,3,0]\n",
    "hard_models = results\n",
    "prob_models = results[results.Model != 'LinearSV']\n",
    "\n",
    "for x in [2,3,5,7,10]:\n",
    "    ECH = EnsembleVoteClassifier([ensemble_models.get(key) for key in hard_models.Model[:x]], voting='hard')\n",
    "    ECS = EnsembleVoteClassifier([ensemble_models.get(key) for key in prob_models.Model[:x]], voting='soft')\n",
    "    print('\\n')\n",
    "    print('{}-Voting Models: 5-fold cross validation:\\n'.format(x))\n",
    "    \n",
    "    for clf, label in zip([ECS, ECH], \n",
    "                          ['{}-VM-Ensemble Soft Voting'.format(x),\n",
    "                           '{}-VM-Ensemble Hard Voting'.format(x)]):\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "        md = clf.fit(X, y)    \n",
    "        clfs.append(md)\n",
    "        \n",
    "        Test_Score = metrics.accuracy_score(clf.predict(X_test), y_test)\n",
    "        print(\"Test Accuracy: %0.2f \" % Test_Score)\n",
    "        \n",
    "        CV_Score = scores.mean()\n",
    "        STDev = scores.std()\n",
    "        \n",
    "        submission = md.predict(test_df)\n",
    "        df = pd.DataFrame({'PassengerId':test_df.index, \n",
    "                               'Survived':submission})\n",
    "        global results\n",
    "        results = results.append({'Model': label,'Para': clf, 'CV Mean': CV_Score,\n",
    "                'Test_Score':Test_Score,'CV STDEV': STDev}, ignore_index=True)\n",
    "        ensemble_models[label] = model\n",
    "        df.to_csv((os.path.join(path,(r\"submissions/{}.csv\".format(label)))),header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Para</th>\n",
       "      <th>Test_Score</th>\n",
       "      <th>CV Mean</th>\n",
       "      <th>CV STDEV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bagger_ensemble</td>\n",
       "      <td>{'n_estimators': 226}</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.813237</td>\n",
       "      <td>0.021745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.811829</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.927374</td>\n",
       "      <td>0.814616</td>\n",
       "      <td>0.024504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient_Boosting</td>\n",
       "      <td>{'learning_rate': 0.16000000000000003, 'max_de...</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.809022</td>\n",
       "      <td>0.034348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.814626</td>\n",
       "      <td>0.024441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>0.027604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.893855</td>\n",
       "      <td>0.813208</td>\n",
       "      <td>0.023742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.821649</td>\n",
       "      <td>0.020950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.823057</td>\n",
       "      <td>0.022203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.813228</td>\n",
       "      <td>0.017787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBsklearn</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.025407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBstandard</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.816025</td>\n",
       "      <td>0.023170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.816025</td>\n",
       "      <td>0.027418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN</td>\n",
       "      <td>{'n_neighbors': 6, 'weights': 'uniform'}</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.811859</td>\n",
       "      <td>0.032075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVCrbf</td>\n",
       "      <td>{'svc__C': 5364, 'svc__gamma': 0.0027825594022...</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.818832</td>\n",
       "      <td>0.022658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoost_Ensemble</td>\n",
       "      <td>{'learning_rate': 1.6000000000000001, 'n_estim...</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.810401</td>\n",
       "      <td>0.012438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>{'max_depth': 9, 'max_features': 0.65000000000...</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.813208</td>\n",
       "      <td>0.039981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PCA_SVC</td>\n",
       "      <td>{'pca__n_components': 6, 'svc__C': 4902, 'svc_...</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.821639</td>\n",
       "      <td>0.017969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sci_kit XGB</td>\n",
       "      <td>{'max_depth': 28, 'subsample': 0.9055517564303...</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.786506</td>\n",
       "      <td>0.044517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gaussian</td>\n",
       "      <td>GaussianNB(priors=None)</td>\n",
       "      <td>0.821229</td>\n",
       "      <td>0.773184</td>\n",
       "      <td>0.025625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.776536</td>\n",
       "      <td>0.016945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LinearSV</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.803352</td>\n",
       "      <td>0.033594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>StochasticGradientDescent</td>\n",
       "      <td>{'loss': 'squared_hinge'}</td>\n",
       "      <td>0.726257</td>\n",
       "      <td>0.738737</td>\n",
       "      <td>0.067388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RSNeural_Net</td>\n",
       "      <td>{'hidden_layer_sizes': 464, 'early_stopping': ...</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.648892</td>\n",
       "      <td>0.027143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  \\\n",
       "0              Bagger_ensemble   \n",
       "16   2-VM-Ensemble Hard Voting   \n",
       "15   2-VM-Ensemble Soft Voting   \n",
       "1            Gradient_Boosting   \n",
       "17   3-VM-Ensemble Soft Voting   \n",
       "18   3-VM-Ensemble Hard Voting   \n",
       "20   5-VM-Ensemble Hard Voting   \n",
       "21   7-VM-Ensemble Soft Voting   \n",
       "19   5-VM-Ensemble Soft Voting   \n",
       "23  10-VM-Ensemble Soft Voting   \n",
       "2                   XGBsklearn   \n",
       "3                  XGBstandard   \n",
       "22   7-VM-Ensemble Hard Voting   \n",
       "24  10-VM-Ensemble Hard Voting   \n",
       "4                          KNN   \n",
       "7                       SVCrbf   \n",
       "6            AdaBoost_Ensemble   \n",
       "5                Random_Forest   \n",
       "9                      PCA_SVC   \n",
       "8                  Sci_kit XGB   \n",
       "10                    Gaussian   \n",
       "11         Logistic_Regression   \n",
       "12                    LinearSV   \n",
       "13   StochasticGradientDescent   \n",
       "14                RSNeural_Net   \n",
       "\n",
       "                                                 Para  Test_Score   CV Mean  \\\n",
       "0                               {'n_estimators': 226}    0.938547  0.813237   \n",
       "16  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.932961  0.811829   \n",
       "15  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.927374  0.814616   \n",
       "1   {'learning_rate': 0.16000000000000003, 'max_de...    0.921788  0.809022   \n",
       "17  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.921788  0.814626   \n",
       "18  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.916201  0.814607   \n",
       "20  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.893855  0.813208   \n",
       "21  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.882682  0.821649   \n",
       "19  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.882682  0.823057   \n",
       "23  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.865922  0.813228   \n",
       "2   XGBClassifier(base_score=0.5, booster='gbtree'...    0.860335  0.797203   \n",
       "3   XGBClassifier(base_score=0.5, booster='gbtree'...    0.860335  0.832402   \n",
       "22  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.860335  0.816025   \n",
       "24  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.854749  0.816025   \n",
       "4            {'n_neighbors': 6, 'weights': 'uniform'}    0.854749  0.811859   \n",
       "7   {'svc__C': 5364, 'svc__gamma': 0.0027825594022...    0.837989  0.818832   \n",
       "6   {'learning_rate': 1.6000000000000001, 'n_estim...    0.837989  0.810401   \n",
       "5   {'max_depth': 9, 'max_features': 0.65000000000...    0.837989  0.813208   \n",
       "9   {'pca__n_components': 6, 'svc__C': 4902, 'svc_...    0.832402  0.821639   \n",
       "8   {'max_depth': 28, 'subsample': 0.9055517564303...    0.832402  0.786506   \n",
       "10                            GaussianNB(priors=None)    0.821229  0.773184   \n",
       "11  LogisticRegression(C=1.0, class_weight=None, d...    0.815642  0.776536   \n",
       "12  LinearSVC(C=1.0, class_weight=None, dual=True,...    0.815642  0.803352   \n",
       "13                          {'loss': 'squared_hinge'}    0.726257  0.738737   \n",
       "14  {'hidden_layer_sizes': 464, 'early_stopping': ...    0.625698  0.648892   \n",
       "\n",
       "    CV STDEV  \n",
       "0   0.021745  \n",
       "16  0.028400  \n",
       "15  0.024504  \n",
       "1   0.034348  \n",
       "17  0.024441  \n",
       "18  0.027604  \n",
       "20  0.023742  \n",
       "21  0.020950  \n",
       "19  0.022203  \n",
       "23  0.017787  \n",
       "2   0.025407  \n",
       "3   0.000000  \n",
       "22  0.023170  \n",
       "24  0.027418  \n",
       "4   0.032075  \n",
       "7   0.022658  \n",
       "6   0.012438  \n",
       "5   0.039981  \n",
       "9   0.017969  \n",
       "8   0.044517  \n",
       "10  0.025625  \n",
       "11  0.016945  \n",
       "12  0.033594  \n",
       "13  0.067388  \n",
       "14  0.027143  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by=[\"Test_Score\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Generalization\n",
    "Stacked generalized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 8) (891,) (418, 8)\n",
      "5-fold cross validation:\n",
      "\n",
      "##### Base Model 0 #####\n",
      "Train CV Accuracy: 0.80 (+/- 0.02)\n",
      "Train Accuracy: 0.85 \n",
      "Test Accuracy: 0.87 \n",
      "##### Base Model 1 #####\n",
      "Train CV Accuracy: 0.81 (+/- 0.02)\n",
      "Train Accuracy: 0.92 \n",
      "Test Accuracy: 0.84 \n",
      "##### Base Model 2 #####\n",
      "Train CV Accuracy: 0.82 (+/- 0.02)\n",
      "Train Accuracy: 0.83 \n",
      "Test Accuracy: 0.85 \n",
      "##### Meta Model #####\n",
      "Train CV Accuracy: 0.92 (+/- 0.01)\n",
      "Train Accuracy: 0.92 \n",
      "Test Accuracy: 0.83 \n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "X = train_df.drop([\"Survived\"] , axis=1)\n",
    "y = train_df[\"Survived\"]\n",
    "\n",
    "#test_df  = test_df.drop([\"PassengerId\"] , axis=1).copy()\n",
    "print(X.shape, y.shape, test_df.shape)\n",
    "\n",
    "#Normalize\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2017)\n",
    "\n",
    "kfold = cross_validation.StratifiedKFold(y=y_train, n_folds=5, random_state=2017)\n",
    "num_trees = 10\n",
    "verbose = True # to print the progress\n",
    "\n",
    "clfs = [KNeighborsClassifier(),\n",
    "        RandomForestClassifier(n_estimators=num_trees, random_state=2017),\n",
    "        GradientBoostingClassifier(n_estimators=num_trees, random_state=2017)]\n",
    "\n",
    "# Creating train and test sets for blending\n",
    "dataset_blend_train = np.zeros((X_train.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_test.shape[0], len(clfs)))\n",
    "dataset_blend_test_df = np.zeros((test_df.shape[0], len(clfs)))\n",
    "\n",
    "print('5-fold cross validation:\\n')\n",
    "for i, clf in enumerate(clfs):   \n",
    "    scores = cross_validation.cross_val_score(clf, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    print(\"##### Base Model %0.0f #####\" % i)\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "    clf.fit(X_train, y_train)   \n",
    "    print(\"Train Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_train), y_train)))\n",
    "    dataset_blend_train[:,i] = clf.predict_proba(X_train)[:, 1]\n",
    "    dataset_blend_test[:,i] = clf.predict_proba(X_test)[:, 1]\n",
    "    dataset_blend_test_df[:,i] = clf.predict_proba(test_df)[:, 1]\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))    \n",
    "\n",
    "print(\"##### Meta Model #####\")\n",
    "clf = LogisticRegression()\n",
    "scores = cross_validation.cross_val_score(clf, dataset_blend_train, y_train, cv=kfold, scoring='accuracy')\n",
    "clf.fit(dataset_blend_train, y_train)\n",
    "print(\"Train CV Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "print(\"Train Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(dataset_blend_train), y_train)))\n",
    "print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(dataset_blend_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.78 (+/- 0.03) [stacked]\n",
      "Test_Score: 0.821229050279\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(clf, X, y, cv=cv, scoring='accuracy')\n",
    "norm_save(clf, score, \"stacked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modelname= \"stacked\"\n",
    "# submission = clf.predict(dataset_blend_test_df)\n",
    "# df = pd.DataFrame({'PassengerId':test_df.index, 'Survived':submission})\n",
    "# results = results.append({'Model': modelname,'Para': clf, 'Test_Score':(metrics.accuracy_score(clf.predict(dataset_blend_test), y_test))}, ignore_index=True)\n",
    "# df.to_csv((os.path.join(path,(\"submissions/{}.csv\".format(modelname)))),header=True,index=False)\n",
    "# with open((os.path.join(path,(r\"Pickle/{}.pickle\".format(modelname)))), 'wb') as f: pickle.dump(model, f)\n",
    "# len(clf.predict(dataset_blend_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Para</th>\n",
       "      <th>Test_Score</th>\n",
       "      <th>CV Mean</th>\n",
       "      <th>CV STDEV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bagger_ensemble</td>\n",
       "      <td>{'n_estimators': 226}</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.813237</td>\n",
       "      <td>0.021745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.811829</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.927374</td>\n",
       "      <td>0.814616</td>\n",
       "      <td>0.024504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient_Boosting</td>\n",
       "      <td>{'learning_rate': 0.16000000000000003, 'max_de...</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.809022</td>\n",
       "      <td>0.034348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.814626</td>\n",
       "      <td>0.024441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>0.027604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.893855</td>\n",
       "      <td>0.813208</td>\n",
       "      <td>0.023742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.821649</td>\n",
       "      <td>0.020950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.823057</td>\n",
       "      <td>0.022203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.813228</td>\n",
       "      <td>0.017787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.816025</td>\n",
       "      <td>0.023170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBstandard</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBsklearn</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.025407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN</td>\n",
       "      <td>{'n_neighbors': 6, 'weights': 'uniform'}</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.811859</td>\n",
       "      <td>0.032075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[BaggingClassifier...</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.816025</td>\n",
       "      <td>0.027418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVCrbf</td>\n",
       "      <td>{'svc__C': 5364, 'svc__gamma': 0.0027825594022...</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.818832</td>\n",
       "      <td>0.022658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoost_Ensemble</td>\n",
       "      <td>{'learning_rate': 1.6000000000000001, 'n_estim...</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.810401</td>\n",
       "      <td>0.012438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>{'max_depth': 9, 'max_features': 0.65000000000...</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.813208</td>\n",
       "      <td>0.039981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PCA_SVC</td>\n",
       "      <td>{'pca__n_components': 6, 'svc__C': 4902, 'svc_...</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.821639</td>\n",
       "      <td>0.017969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sci_kit XGB</td>\n",
       "      <td>{'max_depth': 28, 'subsample': 0.9055517564303...</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.786506</td>\n",
       "      <td>0.044517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>stacked</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.821229</td>\n",
       "      <td>0.781006</td>\n",
       "      <td>0.026769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gaussian</td>\n",
       "      <td>GaussianNB(priors=None)</td>\n",
       "      <td>0.821229</td>\n",
       "      <td>0.773184</td>\n",
       "      <td>0.025625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LinearSV</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.803352</td>\n",
       "      <td>0.033594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.776536</td>\n",
       "      <td>0.016945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>StochasticGradientDescent</td>\n",
       "      <td>{'loss': 'squared_hinge'}</td>\n",
       "      <td>0.726257</td>\n",
       "      <td>0.738737</td>\n",
       "      <td>0.067388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RSNeural_Net</td>\n",
       "      <td>{'hidden_layer_sizes': 464, 'early_stopping': ...</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.648892</td>\n",
       "      <td>0.027143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  \\\n",
       "0              Bagger_ensemble   \n",
       "16   2-VM-Ensemble Hard Voting   \n",
       "15   2-VM-Ensemble Soft Voting   \n",
       "1            Gradient_Boosting   \n",
       "17   3-VM-Ensemble Soft Voting   \n",
       "18   3-VM-Ensemble Hard Voting   \n",
       "20   5-VM-Ensemble Hard Voting   \n",
       "21   7-VM-Ensemble Soft Voting   \n",
       "19   5-VM-Ensemble Soft Voting   \n",
       "23  10-VM-Ensemble Soft Voting   \n",
       "22   7-VM-Ensemble Hard Voting   \n",
       "3                  XGBstandard   \n",
       "2                   XGBsklearn   \n",
       "4                          KNN   \n",
       "24  10-VM-Ensemble Hard Voting   \n",
       "7                       SVCrbf   \n",
       "6            AdaBoost_Ensemble   \n",
       "5                Random_Forest   \n",
       "9                      PCA_SVC   \n",
       "8                  Sci_kit XGB   \n",
       "25                     stacked   \n",
       "10                    Gaussian   \n",
       "12                    LinearSV   \n",
       "11         Logistic_Regression   \n",
       "13   StochasticGradientDescent   \n",
       "14                RSNeural_Net   \n",
       "\n",
       "                                                 Para  Test_Score   CV Mean  \\\n",
       "0                               {'n_estimators': 226}    0.938547  0.813237   \n",
       "16  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.932961  0.811829   \n",
       "15  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.927374  0.814616   \n",
       "1   {'learning_rate': 0.16000000000000003, 'max_de...    0.921788  0.809022   \n",
       "17  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.921788  0.814626   \n",
       "18  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.916201  0.814607   \n",
       "20  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.893855  0.813208   \n",
       "21  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.882682  0.821649   \n",
       "19  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.882682  0.823057   \n",
       "23  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.865922  0.813228   \n",
       "22  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.860335  0.816025   \n",
       "3   XGBClassifier(base_score=0.5, booster='gbtree'...    0.860335  0.832402   \n",
       "2   XGBClassifier(base_score=0.5, booster='gbtree'...    0.860335  0.797203   \n",
       "4            {'n_neighbors': 6, 'weights': 'uniform'}    0.854749  0.811859   \n",
       "24  EnsembleVoteClassifier(clfs=[BaggingClassifier...    0.854749  0.816025   \n",
       "7   {'svc__C': 5364, 'svc__gamma': 0.0027825594022...    0.837989  0.818832   \n",
       "6   {'learning_rate': 1.6000000000000001, 'n_estim...    0.837989  0.810401   \n",
       "5   {'max_depth': 9, 'max_features': 0.65000000000...    0.837989  0.813208   \n",
       "9   {'pca__n_components': 6, 'svc__C': 4902, 'svc_...    0.832402  0.821639   \n",
       "8   {'max_depth': 28, 'subsample': 0.9055517564303...    0.832402  0.786506   \n",
       "25  LogisticRegression(C=1.0, class_weight=None, d...    0.821229  0.781006   \n",
       "10                            GaussianNB(priors=None)    0.821229  0.773184   \n",
       "12  LinearSVC(C=1.0, class_weight=None, dual=True,...    0.815642  0.803352   \n",
       "11  LogisticRegression(C=1.0, class_weight=None, d...    0.815642  0.776536   \n",
       "13                          {'loss': 'squared_hinge'}    0.726257  0.738737   \n",
       "14  {'hidden_layer_sizes': 464, 'early_stopping': ...    0.625698  0.648892   \n",
       "\n",
       "    CV STDEV  \n",
       "0   0.021745  \n",
       "16  0.028400  \n",
       "15  0.024504  \n",
       "1   0.034348  \n",
       "17  0.024441  \n",
       "18  0.027604  \n",
       "20  0.023742  \n",
       "21  0.020950  \n",
       "19  0.022203  \n",
       "23  0.017787  \n",
       "22  0.023170  \n",
       "3   0.000000  \n",
       "2   0.025407  \n",
       "4   0.032075  \n",
       "24  0.027418  \n",
       "7   0.022658  \n",
       "6   0.012438  \n",
       "5   0.039981  \n",
       "9   0.017969  \n",
       "8   0.044517  \n",
       "25  0.026769  \n",
       "10  0.025625  \n",
       "12  0.033594  \n",
       "11  0.016945  \n",
       "13  0.067388  \n",
       "14  0.027143  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by=[\"Test_Score\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model: Soft Voting Ensemble with Top Seven Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.91      0.88       109\n",
      "          1       0.84      0.76      0.80        70\n",
      "\n",
      "avg / total       0.85      0.85      0.85       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = train_df.drop([\"Survived\"] , axis=1)\n",
    "y = train_df[\"Survived\"]\n",
    "\n",
    "# use train/test split with different random_state values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "\n",
    "evalmodel = EnsembleVoteClassifier([ensemble_models.get(key) for key in prob_models.Model[:7]], voting='soft')\n",
    "evalmodel.fit(X_train, y_train)\n",
    "y_pred = evalmodel.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Output\n",
    "clf = EnsembleVoteClassifier([ensemble_models.get(key) for key in prob_models.Model[:7]], voting='soft')\n",
    "md = clf.fit(X, y)\n",
    "df = pd.DataFrame({'PassengerId':test_df.index, 'Survived':md.predict(test_df)})\n",
    "df.to_csv((os.path.join(path,(\"submissions/{}.csv\".format(\"Soft_Voting_7_TopModel\")))),header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model took 6337.77 seconds to train\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Model took %0.2f seconds to train\"%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reflection on model\n",
    "\n",
    "Ensemble models suggest that testing accuracy is in the high 80s, however when applied to Kaggle out of sample data, perfromance is consistently in the high 70s. This suggests that either the submission data is very different, or that my model is overfitting on the given data.\n",
    "\n",
    " - May want to explore models with a greater emphasis on randomness in order to tone down the overfitting.\n",
    " - Perhaps compare variable distribution between submission data and training data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Number Recognition Data Load\n",
    "\n",
    "path = r\"C:/Users/Nicol/Google Drive/Learning/Jupyter/Number_Recognition\"\n",
    "train_df = pd.read_csv(open(os.path.join(path, \"train.csv\"), \"r\")) \n",
    "test_df = pd.read_csv(open(os.path.join(path, \"test.csv\"), \"r\")) \n",
    "\n",
    "train_df = train_df.take(np.random.permutation(len(train_df)))\n",
    "\n",
    "X = train_df.iloc[:,1:]\n",
    "y = train_df.iloc[:,0]\n",
    "print(X.shape, y.shape, test_df.shape)\n",
    "\n",
    "results=[]\n",
    "def save(model, modelname):\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(test_df)\n",
    "\n",
    "    df = pd.DataFrame({'ImageId':list(range(1, len(submission)+1)), \n",
    "                       'Label':submission})\n",
    "\n",
    "    print(len(df))\n",
    "    \n",
    "    df.to_csv((os.path.join(path,(\"submissions/{}.csv\".format(modelname)))),header=True,index=False)\n",
    "    \n",
    "    #CV and Save Scores\n",
    "    trainingscore = (grid.best_score_*100)\n",
    "    results.append([(trainingscore),(\"{}\".format(modelname)), grid.best_estimator_])\n",
    "    print(trainingscore)\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    with open((os.path.join(path,(r\"Pickle/{}.pickle\".format(modelname)))), 'wb') as f: pickle.dump(model, f)\n",
    "\n",
    "#Ensemble\n",
    "\n",
    "def ensembling(model, modelname):\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(test_df)\n",
    "    df = pd.DataFrame({'ImageId':list(range(1, len(submission)+1)), \n",
    "                       'Label':submission})\n",
    "    print(len(df))\n",
    "    df.to_csv((os.path.join(path(\"submissions/{}.csv\".format(modelname)))),header=True,index=False)\n",
    "    with open((os.path.join(path,(r\"Pickle/{}.pickle\".format(modelname)))), 'wb') as f: pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
