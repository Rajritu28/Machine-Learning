{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Classification Problem:\n",
    "## Machine Learning, Randomized Tuning, Ensemble Voting, Pipelines, and Stacking Models\n",
    "## Generative/ Discriminative Models, Nonparametric and Parametric Models, Dimensionality Reduction, Regularization\n",
    "\n",
    "Author: Nick Brooks \n",
    "\n",
    "First Posted: November 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Introduction](#Intro)\n",
    "1. [Feature Engineering](#FE)\n",
    "1. [Helper Functions](#HF)\n",
    "1. >[**Discriminative Models**](#Discriminative)\n",
    "1. >[**Non-Parametric**](#Non-Parametric)\n",
    "1. [Introduction to Confusion Matrix](#Conf)\n",
    "1. [K-Nearest Neighbors](#KNN)\n",
    "1. [Stochastic Gradient Descent](#SGD)\n",
    "1. [Decision Trees](#Tree)\n",
    "1. [Introduction to Feature Importance Graphic](#FIG)\n",
    "1. [Ensemble Methods for Decision Trees](#EMDT)\n",
    "1. [Bootstrap Ensemble](#Bootstrap)\n",
    "1. [Random Forest](#RandomForest)\n",
    "1. [Adaptive Boosting](#Adaboost)\n",
    "1. [Gradient Boosting](#Gradient Boost)\n",
    "1. [eXtreme Gradient Boosting](#XGBOOST)\n",
    "1. >[**Parametric Models**](#Parametric)\n",
    "1. [Logistic Regression](#Logistic)\n",
    "1. [Feedforward Neural Network](#FNN)\n",
    "1. [Introduction to Support Vector Classifier](#SVC)\n",
    "1. [Linear Support Vector Classifier](#LSVC)\n",
    "1. [Radial Basis Function](#RBF)\n",
    "1. [Pipeline and Principal Components Analysis](#PCA)\n",
    "1. >[**Generative Parametric Models**](#GPM)\n",
    "1. [Gaussian Naive Bayes](#GNB)\n",
    "1. >Higher Level\n",
    "1. [Voting Ensembles](#Vote)\n",
    "1. [Stacking](#Stack)\n",
    "1. [Table of Results](#TOR)\n",
    "1. [Correlating Outputs](#COR)\n",
    "1. [Reflection](#REFL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<a id=\"Intro\"></a>\n",
    "\n",
    "**Outsiders introduction to the Titanic Machine Learning Challenge:**\n",
    "The Titanic Disaster Dataset is based on the sinking of the RMS Titanic on April 14 1912, the largest passenger liner at the time. 1500 out of 2224 of the passengers died in the disaster. The dataset is a collection of the passenger’s name, demographics, cabin class, ticket price, port boarded, and family information. Out of the 2224 passengers onboard, only 1237 are included in the dataset. Out of this 1237, only 891 of the passengers have their fate in the disaster declared. So, whether 418 passengers died or survived is an artificial mystery!\n",
    "\n",
    "Within Machine learning, Supervised Learning is the process of enabling a computer algorithm to model a set of characteristics to a certain outcome. In the case of this project, the characteristics are the information on each passenger (as stated in the last paragraph), and the outcome is whether the passenger died. It is called *Supervised* because the model learns the relationship by studying characteristic and their outcome of similar events, with the goal of figuring out the outcome of unlabeled event. In this case, the 418 passengers whose outcome is not known. This challenge is the quintessential supervised classification problem, and has served to introduce many to this craft, which has many powerful applications in the real world.\n",
    "\n",
    "\n",
    "** What I will cover:**\n",
    "The goal of this notebook is to showcase the wide range of models available through the Sklearn wrapper and how to tune them using randomized search. *Note:* In practice, it is usually better to focus one’s energy on a single high performance model, and thoroughly develop its hyperparameters, and of course ensure quality feature selection and engineering. But that is not the goal of this notebook!\n",
    "\n",
    "Since such a range of models are explored, I also take the opportunity to explain the axis through which these models differ: **Parametric vs Non-Parametric**, and **Generative vs. Discriminative**\n",
    "\n",
    "At the end, I also add more flavor to the modeling process by including **Ensemble Voting**, **Pipelining Dimensionality Reduction**, and **Stacking**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updates\n",
    "- **UPDATE 1**: Fixing the best mode for underperformance, uploaded wrong code! [81.339 score not resovled, sorry]\n",
    "- **UPDATE 2**: Cannot reproduce optimal score, will have to gear notebook more towards reproducibility in the future.\n",
    "- **UPDATE 3**: Added Confusion Matrix, ROC curves and Reproducibility through random_state/\n",
    "- **UPDATE 4**: Improved Stacking, fixed non-probabilistic voting, and added outsider introduction\n",
    "- **UPDATE 5**: Added Correlation matrix for predictions, and Table of Content\n",
    "\n",
    "## UpNext\n",
    "- To counter high false negatives, perhaps target *Specificity* (True Negative Rate) with certain models, and ensemble? Wonder what the tradeoff is. Could also experiment with the missclassification by ID (Observation). I tried stacking to get around this problem, but it still persists.\n",
    "- I also want to put more work and research into stacking, since I am (still) utilizing it sub-optimally. \n",
    "- Add shrinkage methods, or dimensionality reduction to reduce redundancy. Since the data is sparse, L1 regularization is better.\n",
    "- Add correlation matrix for the submissions.\n",
    "- Try the Box Cox transformation for continuous variables, instead of sklearn's standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scikitplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e22992d8b2fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscikitplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mskplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Supervised Machine Learning Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scikitplot'"
     ]
    }
   ],
   "source": [
    "# General Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import os\n",
    "import re\n",
    "# import multiprocessing\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic('matplotlib inline')\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "import scikitplot as skplt\n",
    "\n",
    "# Supervised Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import feature_selection\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier # <3\n",
    "\n",
    "# Unsupervised Models\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Evalaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Grid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats as st\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Esemble Voting\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Stacking\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import platform\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version      : 3.6.1\n",
      "Compiler     : GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)\n",
      "Build        : ('default', 'May 11 2017 13:04:09')\n",
      "\n",
      "Current date and time using isoformat:\n",
      "2017-11-30T15:23:53.425070\n"
     ]
    }
   ],
   "source": [
    "print('Version      :', platform.python_version())\n",
    "print('Compiler     :', platform.python_compiler())\n",
    "print('Build        :', platform.python_build())\n",
    "\n",
    "print(\"\\nCurrent date and time using isoformat:\")\n",
    "print(datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Master Parameters:\n",
    "n_splits = 5 # Cross Validation Splits\n",
    "n_iter = 90 # Randomized Search Iterations\n",
    "scoring = 'accuracy' # Model Selection during Cross-Validation\n",
    "rstate = 25 # Random State used \n",
    "testset_size = 0.30\n",
    "\n",
    "# Trees Parameters\n",
    "# lower_tree_range = 300\n",
    "# upper_tree_range = 1000\n",
    "n_tree_range = st.randint(600, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Pre-Processing\n",
    "### Cleaning:\n",
    "- Category to Numerical representation\n",
    "- Dealing with Missing Values, and Filling NA\n",
    "\n",
    "\n",
    "### Feature Engineering:\n",
    "<a id=\"FE\"></a>\n",
    "- Extracting Titles from name variable to use as feature\n",
    "- Rescaling continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "train_df = pd.read_csv(\"../input/train.csv\", index_col='PassengerId')\n",
    "test_df = pd.read_csv(\"../input/test.csv\", index_col='PassengerId')\n",
    "\n",
    "# For Pre-Processing, combine train/test to simultaneously apply transformations\n",
    "Survived = train_df['Survived'].copy()\n",
    "train_df = train_df.drop('Survived', axis=1).copy()\n",
    "df = pd.concat([test_df, train_df])\n",
    "traindex = train_df.index\n",
    "testdex = test_df.index\n",
    "del train_df\n",
    "del test_df\n",
    "\n",
    "# New Variables engineering, heavily influenced by:\n",
    "# Kaggle Source- https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n",
    "# Family Size\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "# Name Length\n",
    "df['Name_length'] = df['Name'].apply(len)\n",
    "# Is Alone?\n",
    "df['IsAlone'] = 0\n",
    "df.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "# Title: (Source)\n",
    "# Kaggle Source- https://www.kaggle.com/ash316/eda-to-prediction-dietanic\n",
    "df['Title']=0\n",
    "df['Title']=df.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n",
    "df['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col',\n",
    "                         'Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n",
    "\n",
    "# Age\n",
    "df.loc[(df.Age.isnull())&(df.Title=='Mr'),'Age']= df.Age[df.Title==\"Mr\"].mean()\n",
    "df.loc[(df.Age.isnull())&(df.Title=='Mrs'),'Age']= df.Age[df.Title==\"Mrs\"].mean()\n",
    "df.loc[(df.Age.isnull())&(df.Title=='Master'),'Age']= df.Age[df.Title==\"Master\"].mean()\n",
    "df.loc[(df.Age.isnull())&(df.Title=='Miss'),'Age']= df.Age[df.Title==\"Miss\"].mean()\n",
    "df.loc[(df.Age.isnull())&(df.Title=='Other'),'Age']= df.Age[df.Title==\"Other\"].mean()\n",
    "df = df.drop('Name', axis=1)\n",
    "\n",
    "# Fill NA\n",
    "# Categoricals Variable\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode().iloc[0])\n",
    "# Continuous Variable\n",
    "df['Fare'] = df['Fare'].fillna(df['Fare'].mean())\n",
    "\n",
    "## Assign Binary to Sex str\n",
    "df['Sex'] = df['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n",
    "# Title\n",
    "#df['Title'] = df['Title'].map( {'Mr': 0, 'Mrs': 1, 'Miss': 2, 'Master':3, 'Rare':4} ).astype(int)\n",
    "# Embarked\n",
    "df['Embarked'] = df['Embarked'].map( {'Q': 0, 'S': 1, 'C': 2} ).astype(int)\n",
    "\n",
    "# Get Rid of Ticket and Cabin Variable\n",
    "df= df.drop(['Ticket', 'Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Before Applying dummy variables, I take the opportunity to look at distributions and correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "pd.concat([df.loc[traindex, :], Survived], axis=1).hist()\n",
    "plt.show()\n",
    "\n",
    "# Correlation Plot\n",
    "sns.heatmap(pd.concat([df.loc[traindex, :], Survived], axis=1).corr(), annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scaling between -1 and 1. Good practice for continuous variables.\n",
    "from sklearn import preprocessing\n",
    "for col in ['Fare','Age','Name_length']:\n",
    "    transf = df[col].reshape(-1,1)\n",
    "    scaler = preprocessing.StandardScaler().fit(transf)\n",
    "    df[col] = scaler.transform(transf)\n",
    "\n",
    "# Finish Pre-Processing\n",
    "# Dummmy Variables (One Hot Encoding)\n",
    "df = pd.get_dummies(df, columns=['Embarked','Title','Parch','SibSp','Pclass'], )\n",
    "\n",
    "# Now that pre-processing is complete, split data into train/test again.\n",
    "train_df = df.loc[traindex, :]\n",
    "train_df['Survived'] = Survived\n",
    "test_df = df.loc[testdex, :]\n",
    "\n",
    "# Dead Weight\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Your Position!\n",
    "Declaring dependent and independent variables as well as helper functions that record the mean cross-validation average of model accuracy, and its standard deviation to understand the volatility of the models.\n",
    "\n",
    "Also includes remnants of my previous \"write submission\" system for those interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Depedent and Indepedent Variables\n",
    "X = train_df.drop([\"Survived\"] , axis=1)\n",
    "y = train_df[\"Survived\"]\n",
    "print(\"X, Y, Test Shape:\",X.shape, y.shape, test_df.shape) # Data Dimensions\n",
    "\n",
    "# Storage for Model and Results\n",
    "results = pd.DataFrame(columns=['Model','Para','Test_Score','CV Mean','CV STDEV'])\n",
    "ensemble_models= {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Dependent Variable\n",
    "This signifies that there is an unequal occurrence of the dependent variable \"Survived\". This could potentially lead to a flawed model. I deal with this by stratifying the train/test groups, leading to equal representation of classes. Additional methods to handle imbalanced datasets include data augmentation and re-sampling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Depedent Variable Distribution\")\n",
    "print(y.value_counts(normalize=True)*100)\n",
    "print(\"0 = Died\", \"\\n1 = Survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Feature Importance\n",
    "- [PCA](https://machinelearningmastery.com/feature-selection-machine-learning-python/)\n",
    "- [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "\n",
    "### Dimensionality Reduction: Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Feature Count (With One Hot Encoding):\", X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [2,4,6,8,10,12]\n",
    "for x in levels:\n",
    "    pca = PCA(n_components=x)\n",
    "    fit = pca.fit(train_df)\n",
    "\n",
    "    print((\"{} Components \\nExplained Variance: {}\\n\").format(x, fit.explained_variance_ratio_))\n",
    "    #print(fit.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reduce Dimensionality\n",
    "pca = PCA(n_components=5)\n",
    "fit = pca.fit(X)\n",
    "X = pd.DataFrame(fit.transform(X))\n",
    "test_df = pd.DataFrame(fit.transform(test_df))\n",
    "sns.heatmap(pd.concat([X, Survived], axis=1).corr(), annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "Declaring dependent and independent variables, as well as preparing the training data into its train-validation-test sets for proper modelling. My validation step is executed during the model fitting process.\n",
    "\n",
    "Note: Determining model quality through cross-validation is not correct if hyper-parameter tuning is applied, since this leads to a (hyper-parameter) overfitted evaluation of the model. Use an additional untouched test set.\n",
    "\n",
    "Stratified cross validation splits are used. Number of splits is declared at the start of notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stratified Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testset_size, stratify=y,random_state=rstate)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "\n",
    "# Stratified Cross-Validation\n",
    "cv = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=rstate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "<a id=\"HF\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f01e313e4c0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Survived\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Survived\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to \n",
    "def save(model, modelname):\n",
    "    global results\n",
    "    # Once best model is found, establish more evaluation metrics.\n",
    "    model.best_estimator_.fit(X_train, y_train)\n",
    "    \n",
    "    # submission = model.predict(test_df)\n",
    "    # df = pd.DataFrame({'PassengerId':test_df.index,'Survived':submission})\n",
    "    # path = ...\n",
    "    # df.to_csv((os.path.join(path,(\"submissions/{}.csv\".format(modelname)))),header=True,index=False)\n",
    "    \n",
    "    scores = cross_val_score(model.best_estimator_, X_train, y_train, cv=5,\n",
    "                             scoring=scoring, verbose =0)\n",
    "    CV_scores = scores.mean()\n",
    "    STDev = scores.std()\n",
    "    Test_scores = model.score(X_test, y_test)\n",
    "\n",
    "    # CV and Save scores\n",
    "    results = results.append({'Model': modelname,'Para': model.best_params_,'Test_Score': Test_scores,\n",
    "                             'CV Mean':CV_scores, 'CV STDEV': STDev}, ignore_index=True)\n",
    "    ensemble_models[modelname] = model.best_estimator_\n",
    "    \n",
    "    # Print Evaluation\n",
    "    print(\"\\nEvaluation Method: {}\".format(scoring))\n",
    "    print(\"Optimal Model Parameters: {}\".format(grid.best_params_))\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (CV_scores, STDev, modelname))\n",
    "    print('Test_Score:', Test_scores)\n",
    "        \n",
    "    # Scikit Confusion Matrix\n",
    "    model.best_estimator_.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    skplt.metrics.plot_confusion_matrix(y_test, pred, title=\"{} Confusion Matrix\".format(modelname),\n",
    "                normalize=True,figsize=(6,6),text_fontsize='large')\n",
    "    plt.show()\n",
    "    # Colors https://matplotlib.org/examples/color/colormaps_reference.html\n",
    "\n",
    "def norm_save(model,score, modelname):\n",
    "    global results\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(test_df)\n",
    "    df = pd.DataFrame({'PassengerId':test_df.index, \n",
    "                           'Survived':submission})\n",
    "    \n",
    "    CV_Score = score.mean()\n",
    "    Test_scores = model.score(X_test, y_test)\n",
    "    STDev = score.std()\n",
    "    \n",
    "    # CV and Save Scores\n",
    "    Test_Score = model.score(X_test, y_test)\n",
    "    results = results.append({'Model': modelname,'Para': model,'Test_Score': Test_scores,\n",
    "                             'CV Mean': CV_Score, 'CV STDEV': STDev}, ignore_index=True)\n",
    "    ensemble_models[modelname] = model\n",
    "    \n",
    "    print(\"\\nEvaluation Method: {}\".format(scoring))\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (CV_Score, STDev, modelname))  \n",
    "    print('Test_Score:', Test_scores)\n",
    "        \n",
    "    #Scikit Confusion Matrix\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    skplt.metrics.plot_confusion_matrix(y_test, pred, title=\"{} Confusion Matrix\".format(modelname),\n",
    "                normalize=True,figsize=(6,6),text_fontsize='large')\n",
    "    plt.show()\n",
    "    \n",
    "# ROC Curve Plot\n",
    "# http://scikit-plot.readthedocs.io/en/stable/metrics.html\n",
    "def eval_plot(model):\n",
    "    skplt.metrics.plot_roc_curve(y_test, model.predict_proba(X_test))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric\n",
    "<a id=\"Non-Parametric\"></a>\n",
    "Counterpart to Parametric models, Parametric models does not make any assumptions about the data generating process’ distribution. For example, in statistical test, Non-Parametric models utilize rank and medians, instead of the mean and variance! On the other hand, they function in a infinite space of parameters, making their name counter-intuitive, but also highlighting their practical approach to representation; enabling them to increase their flexibility indefinitely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative Models\n",
    "<a id=\"Discriminative\"></a>\n",
    "Discriminative models do not attempt to quantify how the data generating process operates, instead, its goal is to slice and dice the data to classify the data, effectively solving the problem by modeling p(y|x). \n",
    "\n",
    "## K-Nearest Neighbors\n",
    "<a id=\"KNN\"></a>\n",
    "The simplest of all machine learning models, a nonparametric method that works well on short and narrow datasets, but seriously struggles in the high dimensional space. It works by using the K nearest point to the predicted point vote on its class (or continuous number when in the regression context). Note: Since this is a instance based learning algorithm, its function is applied locally, and the computation is deferred until the prediction stage. Furthermore, since the data serves as the “map” for new values, the model size may be clunkier than its counterparts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [KNN]\n",
      "Optimal Model Parameters: {'weights': 'uniform', 'n_neighbors': 5}\n",
      "Test_Score: 0.860335195531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    3.7s finished\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters. Since RandomizedSearchCV is used, I use an uniform random interger range for the function to choose from.\n",
    "param_grid ={'n_neighbors': st.randint(1,40),\n",
    "            'weights':['uniform','distance']\n",
    "            }\n",
    "# Hyper-Parameter Tuning with Cross-Validation\n",
    "grid = RandomizedSearchCV(KNeighborsClassifier(),\n",
    "                    param_grid, # Hyper Parameters\n",
    "                    cv=cv, # Cross-Validation splits. Stratified.\n",
    "                    scoring=scoring, # Best-Validation selection metric.\n",
    "                    verbose=1, # Quality of Life. Frequency of model updates\n",
    "                    n_iter=n_iter, # Number of hyperparameter combinations tried.\n",
    "                    random_state=rstate) # Reproducibility \n",
    "\n",
    "# Execute Tuning on entire dataset\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "save(grid, \"KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Confusion Matrix\n",
    "<a id=\"Conf\"></a>\n",
    "\n",
    "This graphic illustrates the nature of the model mistakes by showing the proportion of false negatives to true negatives, and false positives to true positives. Method is also applicable to results with more than just a binary class.\n",
    "\n",
    "False Negative: When the model labels a positive observation as negative. For example, when the doctor thinks a pregnant women is not pregnant.\n",
    "False Positive: When a negative observation is predicted to be positive. When a healthy man is (falsely) told he has cancer.\n",
    "\n",
    "For the type of sensitive problem described in my examples, model builders may emphasize the minimization of one form of error over the other. We may be more willing to tell healthy person they falsely have cancer, than tell a dying person they have nothing to worry about, and miss the opportunity to receive treatment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "<a id=\"SGD\"></a>\n",
    "\n",
    "ERROR 404. Adding.. Beep Bop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['max_iter', 'l1_ratio', 'penalty', 'n_jobs', 'verbose', 'alpha', 'epsilon', 'power_t', 'n_iter', 'average', 'random_state', 'loss', 'eta0', 'fit_intercept', 'tol', 'class_weight', 'shuffle', 'learning_rate', 'warm_start'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGDClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Train CV Accuracy: 0.77 (+/- 0.03) [StochasticGradientDescent]\n",
      "Optimal Model Parameters: {'loss': 'log'}\n",
      "Test_Score: 0.804469273743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "param_grid ={'loss':[\"hinge\",\"log\",\"modified_huber\",\"epsilon_insensitive\",\"squared_epsilon_insensitive\"]\n",
    "            }\n",
    "\n",
    "grid = GridSearchCV(SGDClassifier(),\n",
    "                    param_grid,cv=cv, scoring=scoring,\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "save(grid, \"StochasticGradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "<a id=\"Tree\"></a>\n",
    "I like to think of decision trees as optimizing a series of “If/Then” statements, eventually assigning the value at the tree’s terminal node. Starting from one point at the top, features can then pass the various trials until being assigned its class at the end “leaf” nodes of the tree (technically, it's more like its root tip since the end nodes are visually represented at the bottom of the graph). Trees used here are binary trees, so nodes can only split into two ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper Function to visualize feature importance\n",
    "predictors = [x for x in X.columns if x not in ['Survived']]\n",
    "def feature_imp(model):\n",
    "    MO = model.fit(X_train, y_train)\n",
    "    feat_imp = pd.Series(MO.feature_importances_, predictors).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['random_state', 'max_features', 'presort', 'max_depth', 'max_leaf_nodes', 'criterion', 'min_impurity_decrease', 'class_weight', 'min_samples_split', 'min_weight_fraction_leaf', 'splitter', 'min_samples_leaf', 'min_impurity_split'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTreeClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Feature Importance Graphic\n",
    "<a id=\"FIG\"></a>\n",
    "Since each split in the decision tree distinguishes the dependent variable, splits closer to the root, aka starting point, have optimally been determined to have the greatest splitting effect. The feature importance graphic measures how much splitting impact each feature has. It is important to note that this by no means points to causality, but just like in hierarchical clustering, does point to a nebulous groups. Furthermore, for ensemble tree methods, feature impact is aggregated over all the trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV Accuracy: 0.827932960894\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAExCAYAAACTeL4rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b3f7d40908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Baseline Decision Tree\n",
    "tree = DecisionTreeClassifier()\n",
    "print(\"Mean CV Accuracy:\",cross_val_score(tree, X, y, cv=cv, scoring=scoring).mean())\n",
    "feature_imp(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doesn't work since Dummy Variables are used, will need to stack/melt dataset.\n",
    "\n",
    "# f, ax = plt.subplots(figsize=(9, 6))\n",
    "# s = sns.heatmap(pd.crosstab(train_df.Sex, train_df.Title),\n",
    "#             annot=True, fmt=\"d\", linewidths=.5, ax=ax,\n",
    "#                 cbar_kws={'label': 'Count'})\n",
    "# s.set_title('Title Count by Sex Crosstab Heatmap')\n",
    "# s.set_xticklabels([\"Mr\",\"Mrs\",\"Miss\",\"Master\",\"Rare\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods for Decision Trees\n",
    "<a id=\"EMDT\"></a>\n",
    "\n",
    "Technique where multiple trees are created and then asked to come together and vote on the model’s outcome. Later in this notebook, this same idea is applied to bring together models of different types.\n",
    "\n",
    "The sub categories **Boosting** and **Bootstrap Aggregating** are part of this framework, but differ in terms of how the group of trees are *trained*.\n",
    "\n",
    "## General Hyper-Parameters for Decision Trees and their Ensembles:\n",
    " *Sklearn implementation, but universal theory/parameters*\n",
    "\n",
    "HyperParameters for Tuning:\n",
    "- max_features: This is the random subset of features to be considered for splitting operation, the lower the better to reduce variance. For Classification model, ideal max_features = sqr(n_var). \n",
    "- max_depth: Maximum depth of the tree. Alternate method of control model depth is *max_leaf_nodes*, which limits the number of terminal nodes, effectively limiting the depth of the tree.\n",
    "- n_estimators: Number of trees built before average prediction is made.\n",
    "- min_samples_split: Minimum number of samples required for a node to be split. Small minimum would potentially lead to a “Bushy Tree”, prone to overfitting. According to Analytic Vidhya, should be around 0.5~1% of the datasize.\n",
    "- min_samples_leaf: Minimum number of samples required at the **Terminal Node** of the tree. In Sklearn, an alternative *min_weight_fraction_leaf* is available to use fraction of the data instead of a fixed integer.\n",
    "- random_state: Ensuring consistent random generation, like seed(). Important to consider for comparing models of the same type to ensure a fair comparison. May cause overfitting if random generation is not representative.\n",
    "\n",
    "### Quality of Life:\n",
    "n_jobs: Computer processors utilized. -1 signifies use all processors\n",
    "Verbose: Amount of tracking information printed. 0 = None, 1 = By Cross Validation, 1 < by tree.\n",
    "\n",
    "## Bootstrap aggregating (AKA Bagging) Decision Trees\n",
    "<a id=\"Bootstrap\"></a>\n",
    "\n",
    "Creates a bunch of trees using a subset of the the data for each, while using sampling without replacement, which means that values may be sampled multiple times. When combined with cross-validation, the values not sampled, or “held-out”, can then be used as the validation set. This results in a better generalizing model: Less prone to overfitting while maintaining a high model capacity (synonymous with model complexity and flexibility).\n",
    "\n",
    "This technique introduces a flavor of the Monte Carlo method, which hopes to achieve better accuracy through sampling.\n",
    "\n",
    "Reading:\n",
    "[Tuning the parameters of your Random Forest model by Analtyics Vidhya](https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.81 (+/- 0.03) [Bagger_ensemble]\n",
      "Optimal Model Parameters: {'n_estimators': 236}\n",
      "Test_Score: 0.944134078212\n"
     ]
    }
   ],
   "source": [
    "# Parameter Tuning\n",
    "param_grid ={'n_estimators': n_tree_range}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "grid = RandomizedSearchCV(BaggingClassifier(tree),\n",
    "                    param_grid, cv=cv, scoring=scoring,\n",
    "                    verbose=1,n_iter=n_iter, random_state=rstate)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "save(grid, \"Bagger_ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "<a id=\"RandomForest\"></a>\n",
    "Builds upon Leo Breiman's Bootstrap Aggregation method by adding a random feature selection dimension. \n",
    "More uncorrelated splits, less overemphasis on certain features. Similar to Neural Net’s dropout, since it forces the model to give a large role to less dominant features, leading to a better generalizing model. However, it differs from dropout in the sense that its effect is not repeated and passed over to future iterations of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV Accuracy: 0.816759776536\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAExCAYAAABvbZXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b3f7cf7278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "print(\"Mean CV Accuracy:\",cross_val_score(model, X, y, cv=cv, scoring=scoring).mean())\n",
    "feature_imp(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['random_state', 'verbose', 'n_jobs', 'max_features', 'class_weight', 'max_depth', 'max_leaf_nodes', 'n_estimators', 'warm_start', 'oob_score', 'criterion', 'bootstrap', 'min_impurity_decrease', 'min_samples_split', 'min_weight_fraction_leaf', 'min_samples_leaf', 'min_impurity_split'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.81 (+/- 0.01) [Random_Forest]\n",
      "Optimal Model Parameters: {'max_depth': 10, 'n_estimators': 445, 'max_leaf_nodes': 9, 'max_features': 0.80000000000000027}\n",
      "Test_Score: 0.882681564246\n"
     ]
    }
   ],
   "source": [
    "param_grid ={'max_depth': st.randint(6, 11),\n",
    "             'n_estimators': n_tree_range,\n",
    "             'max_features':np.arange(0.5,.81, 0.05),\n",
    "            'max_leaf_nodes':st.randint(6, 10)}\n",
    "\n",
    "model= RandomForestClassifier()\n",
    "\n",
    "grid = RandomizedSearchCV(model,\n",
    "                    param_grid, cv=cv,\n",
    "                    scoring=scoring,\n",
    "                    verbose=1,n_iter=n_iter, random_state=rstate)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "save(grid, \"Random_Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAExCAYAAABvbZXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4pftamIGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b3f7d400f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_imp(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Family\n",
    "Simply put, boosting models convert weak models into strong models. Essentially, each weak model is trained on a different distribution of the data, enabling it to learn a narrow rule. When combined, it offers a stronger model. Iteratively, subsequent weak models focus on the source of prediction error from the vote of previous weak models, until the accuracy ceiling is reached.\n",
    "\n",
    "Source:\n",
    "[Quick Introduction to Boosting Algorithms in Machine Learning by Analytics Vidhya](https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/)\n",
    "\n",
    "## Adaptive Boosting\n",
    "<a id=\"Adaboost\"></a>\n",
    "\n",
    "Applies weights to all data points and optimizes them using the loss function. Fixes mistakes by assigning high weights to them during iterative process.\n",
    "\n",
    "Iterates through multiple models in order to determine the best boundaries. It relies on using weak models to determine the pattern, and eventually creates a strong combination of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_estimators', 'algorithm', 'base_estimator', 'random_state', 'learning_rate'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaBoostClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.80 (+/- 0.02) [AdaBoost_Ensemble]\n",
      "Optimal Model Parameters: {'n_estimators': 273, 'learning_rate': 1.6000000000000001}\n",
      "Test_Score: 0.899441340782\n"
     ]
    }
   ],
   "source": [
    "param_grid ={'n_estimators':n_tree_range,\n",
    "            'learning_rate':np.arange(.1, 4, .5)}\n",
    "\n",
    "grid = RandomizedSearchCV(AdaBoostClassifier(),\n",
    "                    param_grid,cv=cv, scoring=scoring,\n",
    "                    verbose=1, n_iter=n_iter, random_state=rstate)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "save(grid, \"AdaBoost_Ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAExCAYAAACTeL4rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSQoREdGTpBARET1JChER0ZOkEBERPf8DWVNE6/lhAJwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b3f8150d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_imp(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gradient Boosting Classifier\n",
    "<a id=\"Gradient Boost\"></a>\n",
    "\n",
    "### Additional Gradient Boosting Hyper-Parameters:\n",
    "Learning_rate: How much parameters are updated after each iteration of gradient descent. Low mean smaller steps, most likely to reach the global minimum, although there are cases where this doesn’t always work as intended.\n",
    "N_estimators: Note this is still the number of trees being built, but it within the GBC’s sequential methodology.\n",
    "Subsample: Similar to the Bootstrap Aggregate method, controlling percentage of data utilized for a tree, although the standard is to sample *without* replacement. In this model, operates in similar ways to Stochastic Gradient Descent in Neural Networks, but with large batch sizes. Note this decision trees are still a *Shallow Model*, so it is still profoundly different from Neural Networks.\n",
    "Loss: Function minimized by gradient descent.\n",
    "Init: Initialization of the model internal parameters. May be used to build off another model's’ outcome.\n",
    "\n",
    "As the name suggest, Gradient Boosting iteratively trains models sequentially to minimize Loss, a convex optimization method similar to that seen in Neural Networks.\n",
    "\n",
    "[Gradient boosting machines, a tutorial by Alexey Natekin1 and Alois Knoll](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/)\n",
    "\n",
    "This is a Greedy Algorithm, which makes the most favorable split when it can. This means it's short sighted and will also stop when the loss doesn’t improve.\n",
    "\n",
    "More information:\n",
    "\n",
    "[Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python by Analytics Vidhya](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['random_state', 'verbose', 'subsample', 'max_depth', 'learning_rate', 'presort', 'max_features', 'max_leaf_nodes', 'warm_start', 'criterion', 'n_estimators', 'min_impurity_decrease', 'init', 'loss', 'min_samples_split', 'min_weight_fraction_leaf', 'min_samples_leaf', 'min_impurity_split'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradientBoostingClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:   47.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.82 (+/- 0.03) [Gradient_Boosting]\n",
      "Optimal Model Parameters: {'loss': 'deviance', 'max_depth': 2.5, 'n_estimators': 220, 'learning_rate': 0.26000000000000001}\n",
      "Test_Score: 0.949720670391\n"
     ]
    }
   ],
   "source": [
    "param_grid ={'n_estimators': n_tree_range,\n",
    "            'loss': ['deviance', 'exponential'],\n",
    "            'learning_rate':np.arange(0.01, 0.32,.05),\n",
    "            'max_depth': np.arange(2, 4.1, .5)}\n",
    "\n",
    "grid = RandomizedSearchCV(GradientBoostingClassifier(),\n",
    "                    param_grid,cv=cv,\n",
    "                    scoring=scoring,\n",
    "                    verbose=1, n_iter=n_iter, random_state=rstate)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "save(grid, \"Gradient_Boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAExCAYAAABvbZXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b3f82948d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_imp(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost - eXtreme Gradient Boosting\n",
    "<a id=\"XGBOOST\"></a>\n",
    "As the name suggest, Gradient Boosting on steroids. Infact, multiple steroids:\n",
    "Regularization\n",
    "Computation Speed and Parallel Processing\n",
    "Handles Missing Values\n",
    "Improves on Gradient Boosting ‘*Greedy*’ tendencies. It does this by considering reaching the max depth and retroactively pruning \n",
    "Interruptible and Resumable, as well as Hadoop capabilities.\n",
    "\n",
    "XGBoost is able to approximate the Loss function more efficiently, thereby leading to faster computation and parallel processing. Inside its objective function, it has also incorporated a regularization term, ridge or lasso, to stay clear of unnecessary high dimensional spaces and complexity.\n",
    "\n",
    "### Hyper-Parameters for Tree Classification Booster (Sklearn Wrapper)\n",
    "Min_child_weight: Similar strand to *min_child_leaf*, which controls the minimum number of observation to split to a terminal node. This instead relates to defines the minimum sum of derivatives found in the Hessian (second-order)of all observations required in a child.\n",
    "Gamma: Node is split only if it gives a positive decrease in the loss function. Higher performance regulator of complexity.\n",
    "Max_delta_step: What the tree’s weights can be.\n",
    "Colsample_bylevel: Random Forest characteristic, where it enables subfraction of features to be randomly selected for each tree.\n",
    "\n",
    "### Regularization\n",
    "In the context of GBMs, regularization through shrinkage is available, but applying it to the model with the tree base-learner is different from its traditional coefficient constriction. For GBM Trees, the shrinking decreases the influence of each additional tree in the iterative process, effectively applying a decay of impact over boosting rounds. As a result, it no longer searches as the feature selection method, since it cannot adjust the coefficients of each feature (and potential interaction/ polynomial terms).\n",
    "\n",
    "\n",
    "### Learning Task Parameters:\n",
    "Objective: binary:logistic = Outputs probability for two classes. Multi:softmax = outputs prediction for *num_class* classes. Multi:prob = probability for 3 or more classes.\n",
    "The objective is merely a matter of catering to the data type, although additional protocols on the probabilities may be set up. For example, the highest probability below a certain threshold may be deemed an inadequate score, passed over for human processing. The optimal loss function should cater to the behavior of the data and goal at hand, and usually requires input from the domain knowledge base.\n",
    "\n",
    "The Evaluation Metric (*eval_metric*) has important implications depending on the problem at hand. If the dataset is imbalanced and the minority class is of interest, it is better to use [AUC] Area Under the Curve than accuracy or rmse, since those metrics can get away with assigning the majority class to all, and still get away with high accuracy!\n",
    "\n",
    "\n",
    "More Info:\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "\n",
    "Python Installation: https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_score', 'subsample', 'n_jobs', 'gamma', 'max_delta_step', 'min_child_weight', 'nthread', 'missing', 'objective', 'seed', 'max_depth', 'n_estimators', 'scale_pos_weight', 'booster', 'colsample_bylevel', 'colsample_bytree', 'reg_alpha', 'reg_lambda', 'silent', 'learning_rate', 'random_state'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   20.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.81 (+/- 0.02) [Sci_kit XGB]\n",
      "Optimal Model Parameters: {'max_depth': 7, 'n_estimators': 379, 'subsample': 0.94079925252783325, 'gamma': 0.031213730001611539, 'reg_alpha': 0.005, 'colsample_bytree': 0.84543613529386086, 'learning_rate': 0.01}\n",
      "Test_Score: 0.932960893855\n"
     ]
    }
   ],
   "source": [
    "one_to_left = st.beta(10, 1)  \n",
    "from_zero_positive = st.expon(0, 50)\n",
    "\n",
    "params = {  \n",
    "    \"n_estimators\": n_tree_range,\n",
    "    \"max_depth\": st.randint(2, 8),\n",
    "    \"learning_rate\": [0.01],\n",
    "    #'max_features':'sqrt',\n",
    "    #\"learning_rate\": st.uniform(0.001, 0.1),\n",
    "    \"colsample_bytree\": one_to_left,\n",
    "    \"subsample\": one_to_left,\n",
    "    \"gamma\": st.uniform(0, 3),\n",
    "    #'reg_alpha': from_zero_positive,\n",
    "    #\"min_child_weight\": from_zero_positive,\n",
    "    'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "xgbreg = XGBClassifier(objective= 'binary:logistic', eval_metric=\"auc\",\n",
    "                       nthreads=2)\n",
    "\n",
    "grid = RandomizedSearchCV(xgbreg, params, n_jobs=1, verbose=1, n_iter=n_iter,\n",
    "                          random_state=rstate, scoring=scoring)  \n",
    "grid.fit(X_train,y_train, verbose=False)\n",
    "save(grid, \"Sci_kit XGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAExCAYAAACTeL4rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfwDJiJxjilrQ9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b3f6cc1780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_imp(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUcAAAEUCAYAAAC4U/QyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b3a1c52a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What is going on with Age, Sex and Survival?\n",
    "with sns.axes_style(style='ticks'):\n",
    "    g = sns.factorplot(\"Sex\", \"Age\", \"Survived\", data=train_df, kind=\"box\")\n",
    "    g.set(ylim=(-1,5))\n",
    "    g.set_axis_labels(\"Sex\", \"Age\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define boosting rounds\n",
    "num_rounds = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "\n",
      "xgBoost - CV Train : 0.81\n",
      "xgBoost - Train : 0.82\n",
      "xgBoost - Test : 0.87\n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [XGBsklearn]\n",
      "Test_Score: 0.905027932961\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(n_estimators = num_rounds,\n",
    "                        objective= 'binary:logistic',\n",
    "                     learning_rate=0.01, random_state=rstate, scoring=scoring)\n",
    "\n",
    "# use early_stopping_rounds to stop the cv when there is no score imporovement\n",
    "model.fit(X_train,y_train, early_stopping_rounds=20, eval_set=[(X_test,\n",
    "y_test)], verbose=False)\n",
    "score = cross_val_score(model, X_train,y_train, cv=cv)\n",
    "print(model)\n",
    "print(\"\\nxgBoost - CV Train : %.2f\" % score.mean())\n",
    "print(\"xgBoost - Train : %.2f\" % metrics.accuracy_score(model.predict(X_train), y_train))\n",
    "print(\"xgBoost - Test : %.2f\" % metrics.accuracy_score(model.predict(X_test), y_test))\n",
    "norm_save(model,score, \"XGBsklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Package Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of trees/estimators is 8\n",
      "[0]\ttest-error:0.128492\ttrain-error:0.179775\n",
      "[1]\ttest-error:0.122905\ttrain-error:0.172753\n",
      "[2]\ttest-error:0.134078\ttrain-error:0.176966\n",
      "[3]\ttest-error:0.145251\ttrain-error:0.176966\n",
      "[4]\ttest-error:0.139665\ttrain-error:0.174157\n",
      "[5]\ttest-error:0.128492\ttrain-error:0.168539\n",
      "[6]\ttest-error:0.128492\ttrain-error:0.168539\n",
      "[7]\ttest-error:0.128492\ttrain-error:0.16573\n",
      "XGB - Train : 0.87\n",
      "XGB - Test : 0.83\n",
      "Train CV Accuracy: 0.87 (+/- 0.00) [XGBstandard]\n",
      "Test_Score: 0.905027932961\n"
     ]
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "xgtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# set xgboost params\n",
    "param = {'max_depth': 3,  # the maximum depth of each tree\n",
    "         'objective': 'binary:logistic'}\n",
    "\n",
    "clf_xgb_cv = xgb.cv(param, xgtrain, num_rounds, \n",
    "                    stratified=True, \n",
    "                    nfold=n_splits, \n",
    "                    early_stopping_rounds=20)\n",
    "print(\"Optimal number of trees/estimators is %i\" % clf_xgb_cv.shape[0])\n",
    "\n",
    "watchlist  = [(xgtest,'test'), (xgtrain,'train')]                \n",
    "clf_xgb = xgb.train(param, xgtrain,clf_xgb_cv.shape[0], watchlist)\n",
    "\n",
    "# predict function will produce the probability \n",
    "# so we'll use 0.5 cutoff to convert probability to class label\n",
    "y_train_pred = (clf_xgb.predict(xgtrain, ntree_limit=clf_xgb.best_iteration) > 0.5).astype(int)\n",
    "y_test_pred = (clf_xgb.predict(xgtest, ntree_limit=clf_xgb.best_iteration) > 0.5).astype(int)\n",
    "score= metrics.accuracy_score(y_test_pred, y_test)\n",
    "print(\"\\nXGB - CV Train : %.2f\" % score)\n",
    "print(\"XGB - Train : %.2f\" % metrics.accuracy_score(y_train_pred, y_train))\n",
    "norm_save(model,score, \"XGBstandard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric Models\n",
    "<a id=\"Parametric\"></a>\n",
    "Family of models which makes assumption of the underlying distribution, and attempts to build models on top of it with a fixed number of parameters. Works great when they're assumption is correct and the data behaves itself.\n",
    "\n",
    "# Parametric Discriminative Models\n",
    "I want to take this opportunity to elaborate a bit more on the discriminative/generative dichotomy. An interesting observation by Andrew Ng is that while discriminative classifiers can reach a higher accuracy cap (asymptotic error), it achieves it at a slower rate than its generative counterpart. He does this by comparing *Naive Bayes* and *Logistic Regression*, two generative models. Therefore, for computational and goal solving reasons, it is best to solve for the conditional probability of p(x|y) directly, instead of trying to compute their joint distribution as seen in generative models.\n",
    " \n",
    "Source: https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "<a id=\"Logistic\"></a>\n",
    "The classification regression. In the binary classification problem, the classic linear regression is unable to bound itself between classes [0,1]. So the logistic regression uses the logit from the sigmoid function, which transformed the weighted input into a probability of class being “1”. Although not the best performer, if offers exploratory information and can potentially reveal causal information, if the experiment is setup correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.80 (+/- 0.01) [Logistic_Regression]\n",
      "Test_Score: 0.759776536313\n"
     ]
    }
   ],
   "source": [
    "model= LogisticRegression()\n",
    "score = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "norm_save(LogisticRegression(),score, \"Logistic_Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.445766\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               Survived   No. Observations:                  891\n",
      "Model:                          Logit   Df Residuals:                      884\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Fri, 24 Nov 2017   Pseudo R-squ.:                  0.3306\n",
      "Time:                        17:11:18   Log-Likelihood:                -397.18\n",
      "converged:                       True   LL-Null:                       -593.33\n",
      "                                        LLR p-value:                 1.264e-81\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Pclass        -0.7940      0.057    -13.934      0.000      -0.906      -0.682\n",
      "Sex            2.3054      0.189     12.226      0.000       1.936       2.675\n",
      "Age            0.0892   1.91e+15   4.67e-17      1.000   -3.74e+15    3.74e+15\n",
      "SibSp         -0.3254      0.108     -3.014      0.003      -0.537      -0.114\n",
      "Parch         -0.1189      0.113     -1.050      0.294      -0.341       0.103\n",
      "Fare           0.0892   1.91e+15   4.67e-17      1.000   -3.74e+15    3.74e+15\n",
      "Embarked       0.2369        nan        nan        nan         nan         nan\n",
      "Title          0.4274      0.096      4.443      0.000       0.239       0.616\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Statsmodel doesn't seem to work in kernel.\n",
    "\n",
    "# import statsmodels.api as sm\n",
    "# logit = sm.Logit(y, X) # fit the model\n",
    "# result = logit.fit()\n",
    "# result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks\n",
    "<a id=\"FNN\"></a>\n",
    "The only “Deep Model” out of the mix. This is a characteristic of *Representation Learning*, which effectively grants the model its own feature processing and selection steps, catering to large, complex data with intertwining effects. In computer vision tasks, the convolutional neural networks is able to piece apart corners, colors, patterns and more. Recent research in Style Transfer even suggests that stylistic properties of a picture, such as art, can be extracted, and controlled.  \n",
    "Source: https://arxiv.org/abs/1611.07865\n",
    "\n",
    "A common explanation for Neural Networks is that it is a whole bunch of Logistic Regressions. An important thing to remembers is that the hidden-layers are fully connected, meaning that each input variables has a weight to each node (hidden-unit) in the hidden layer, thereby resulting in a black box with a whole lot of parameters, and a whole lot of matrix multiplications. Finally it's, ironic that one of the most intelligible classifiers can be transformed into the least intelligible! In *Computer Age Statistical Inference* authors Bradley Efron and Trevor Hastie are hopeful for the next Ronald Fisher to come and provide statistical intelligibility to modern day machine learning models, many of which are highly developed computationally, but lacking inferential theory.\n",
    "\n",
    "Model not ideal for such as small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['validation_fraction', 'max_iter', 'tol', 'solver', 'hidden_layer_sizes', 'power_t', 'early_stopping', 'alpha', 'epsilon', 'batch_size', 'momentum', 'verbose', 'nesterovs_momentum', 'warm_start', 'learning_rate_init', 'activation', 'beta_1', 'beta_2', 'shuffle', 'learning_rate', 'random_state'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLPClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Train CV Accuracy: 0.65 (+/- 0.03) [RSNeural_Net]\n",
      "Optimal Model Parameters: {'max_iter': 27, 'hidden_layer_sizes': 215, 'early_stopping': True, 'activation': 'relu', 'alpha': 100, 'learning_rate': 'adaptive'}\n",
      "Test_Score: 0.687150837989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:   13.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Start with a RandomSearchCV to efficiently Narrow the Ballpark\n",
    "param_grid ={'max_iter': np.logspace(1, 5, 10).astype(\"int32\"),\n",
    "             'hidden_layer_sizes': np.logspace(2, 3, 4).astype(\"int32\"),\n",
    "             'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
    "             'learning_rate': ['adaptive'],\n",
    "             'early_stopping': [True],\n",
    "             'alpha': np.logspace(2, 3, 4).astype(\"int32\")\n",
    "            }\n",
    "\n",
    "model = MLPClassifier()\n",
    "\n",
    "grid = RandomizedSearchCV(model,\n",
    "                    param_grid, cv=cv, scoring=scoring,\n",
    "                    verbose=1, n_iter=n_iter, random_state=rstate)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "save(grid, \"FFNeural_Net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier\n",
    "<a id=\"SVC\"></a>\n",
    "Another model originating in Computer Science, the Support Vector Classifier creates a separation between point to determine class. Maximizing the accuracy of the discriminatory protocol.\n",
    "\n",
    "### Hyperparameters:\n",
    "- C: Rigidity and size of the separation line margin. Increasing C leads to a smaller, thus a more complex mode, and everything that comes with it (High variance, Low Bias)!\n",
    "\n",
    "## Linear Classifier:\n",
    "<a id=\"LSVC\"></a>\n",
    "In this case, the separator is linear. Imagine a two dimensional plot with a straight line separating two classes of points, but efficiently scalable up to a high dimensional space (plane, cube… hypercube :-O ? )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['max_iter', 'penalty', 'multi_class', 'C', 'dual', 'verbose', 'random_state', 'intercept_scaling', 'loss', 'fit_intercept', 'tol', 'class_weight'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearSVC().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.79 (+/- 0.02) [LinearSV]\n",
      "Test_Score: 0.810055865922\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "model = LinearSVC()\n",
    "\n",
    "#Fit Model\n",
    "scores= cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\n",
    "norm_save(model, scores, \"LinearSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Basis Function (RBF)\n",
    "<a id=\"RBF\"></a>\n",
    "Here, not only are non-linear boundaries available, the kernel trick is aso introduced, which enables new representations of the data to be formulated, effectively granting the models new dimensions to find better separators in.\n",
    "\n",
    "### Hyperparameter:\n",
    "- Gamma: How far the influence of a single training example reaches, and sort of like a soft boundary with a gradient. \n",
    "- Low Gamma -> Distant fit, High Gamma = Close Fit, Inverse of the radius of influence of samples selected by the model as support vectors.\n",
    "http://pages.cs.wisc.edu/~yliang/cs760/howtoSVM.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['shrinking', 'max_iter', 'cache_size', 'degree', 'gamma', 'coef0', 'verbose', 'C', 'kernel', 'decision_function_shape', 'random_state', 'tol', 'class_weight', 'probability'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.80 (+/- 0.02) [SVCrbf]\n",
      "Optimal Model Parameters: {'svc__gamma': 0.0027825594022071257, 'svc__C': 3053}\n",
      "Test_Score: 0.860335195531\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel= 'rbf', probability=True)\n",
    "\n",
    "model = Pipeline(steps=[('svc', svc)])\n",
    "\n",
    "\n",
    "param_grid = {'svc__C': st.randint(1,10000),\n",
    "              'svc__gamma': np.logspace(1, -7, 10)}\n",
    "\n",
    "grid = RandomizedSearchCV(model, param_grid,\n",
    "                          cv=cv, verbose=1, scoring=scoring,\n",
    "                         n_iter=n_iter, random_state=rstate)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "save(grid, \"SVCrbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline and Principal Components Analysis and Support Vector Classifier\n",
    "<a id=\"PCA\"></a>\n",
    "Pipelines enable multiple models or processes to be chained up and hyper-parameter tuned together. Very powerful tool, especially when uncertain about a model's reaction to processed data, since it may reveal complex, high performing combinations.\n",
    "\n",
    "### Limitations:\n",
    "The radial basis function support vector machine is actually able to leverage high dimensional data through its *kernel trick* to improve performance. For these reasons, it may not actually benefit from this procedure. \n",
    "\n",
    "### Dimensionality Reduction: Principal Component Analysis\n",
    "Decreases the noise by condensing the features into the dimensions of most variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.80 (+/- 0.02) [PCA_SVC]\n",
      "Optimal Model Parameters: {'svc__gamma': 0.0027825594022071257, 'pca__n_components': 7, 'svc__C': 6764}\n",
      "Test_Score: 0.865921787709\n"
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "svc = SVC(kernel= 'rbf',probability=True)\n",
    "\n",
    "model = Pipeline(steps=[('pca',pca),\n",
    "                        ('svc', svc)])\n",
    "\n",
    "\n",
    "param_grid = {'svc__C': st.randint(1,10000),\n",
    "              'svc__gamma': np.logspace(1, -7, 10),\n",
    "             'pca__n_components': st.randint(1,len(X.columns))}\n",
    "\n",
    "grid = RandomizedSearchCV(model, param_grid,\n",
    "                          cv=cv, verbose=1,\n",
    "                         n_iter=n_iter, random_state=rstate, scoring=scoring)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "save(grid, \"PCA_SVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric Generative Classification\n",
    "<a id=\"GPM\"></a>\n",
    "Models the data generating process in order to assign which categorize the features. Process is represented in terms of joint probabilities. Advantageous since it can be used to generate features, but tends to struggle in terms of accuracy and doesn’t scale well into tall and wide datasets. Only Gaussian Bayes used in this category, but additional Generative Models can be explored, such as:\n",
    "\n",
    "Hidden Markov model\n",
    "Probabilistic context-free grammar\n",
    "Averaged one-dependence estimators\n",
    "Latent Dirichlet allocation\n",
    "Restricted Boltzmann machine\n",
    "Generative adversarial networks\n",
    "\n",
    "## Gaussian Naive Bayes\n",
    "<a id=\"GNB\"></a>\n",
    "Through Bayes rule, is able to use conditional probabilities to classify data. Predicts the class by finding the one with the highest Posterior.\n",
    "\n",
    "### Interpretation:\n",
    "After I increased the feature space through feature engineering, model performance dropped from 75% to worst than random. Furthermore, this model gambles that most passengers died. Only thing going for it is its low false negative rate.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.77 (+/- 0.03) [Gaussian]\n",
      "Test_Score: 0.821229050279\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "score = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\n",
    "norm_save(model,score, \"Gaussian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Para</th>\n",
       "      <th>Test_Score</th>\n",
       "      <th>CV Mean</th>\n",
       "      <th>CV STDEV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gradient_Boosting</td>\n",
       "      <td>{'loss': 'deviance', 'max_depth': 2.5, 'n_esti...</td>\n",
       "      <td>0.949721</td>\n",
       "      <td>0.815922</td>\n",
       "      <td>0.034340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagger_ensemble</td>\n",
       "      <td>{'n_estimators': 236}</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.808850</td>\n",
       "      <td>0.025687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sci_kit XGB</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 379, 'subsamp...</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.814504</td>\n",
       "      <td>0.023807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBsklearn</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.806993</td>\n",
       "      <td>0.021936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBstandard</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.871508</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost_Ensemble</td>\n",
       "      <td>{'n_estimators': 273, 'learning_rate': 1.6}</td>\n",
       "      <td>0.899441</td>\n",
       "      <td>0.799089</td>\n",
       "      <td>0.024731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 445, 'max_le...</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.813165</td>\n",
       "      <td>0.014864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PCA_SVC</td>\n",
       "      <td>{'svc__gamma': 0.00278255940221, 'pca__n_compo...</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.804684</td>\n",
       "      <td>0.021931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>{'weights': 'uniform', 'n_neighbors': 5}</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.810308</td>\n",
       "      <td>0.020743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVCrbf</td>\n",
       "      <td>{'svc__gamma': 0.00278255940221, 'svc__C': 3053}</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.804703</td>\n",
       "      <td>0.017722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.801117</td>\n",
       "      <td>0.013955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LinearSV</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.785475</td>\n",
       "      <td>0.019223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StochasticGradientDescent</td>\n",
       "      <td>{'loss': 'log'}</td>\n",
       "      <td>0.804469</td>\n",
       "      <td>0.766833</td>\n",
       "      <td>0.033926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RSNeural_Net</td>\n",
       "      <td>{'max_iter': 27, 'hidden_layer_sizes': 215, 'e...</td>\n",
       "      <td>0.687151</td>\n",
       "      <td>0.647645</td>\n",
       "      <td>0.033133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  \\\n",
       "5           Gradient_Boosting   \n",
       "2             Bagger_ensemble   \n",
       "6                 Sci_kit XGB   \n",
       "7                  XGBsklearn   \n",
       "8                 XGBstandard   \n",
       "4           AdaBoost_Ensemble   \n",
       "3               Random_Forest   \n",
       "13                    PCA_SVC   \n",
       "0                         KNN   \n",
       "12                     SVCrbf   \n",
       "9         Logistic_Regression   \n",
       "11                   LinearSV   \n",
       "1   StochasticGradientDescent   \n",
       "10               RSNeural_Net   \n",
       "\n",
       "                                                 Para  Test_Score   CV Mean  \\\n",
       "5   {'loss': 'deviance', 'max_depth': 2.5, 'n_esti...    0.949721  0.815922   \n",
       "2                               {'n_estimators': 236}    0.944134  0.808850   \n",
       "6   {'max_depth': 7, 'n_estimators': 379, 'subsamp...    0.932961  0.814504   \n",
       "7   XGBClassifier(base_score=0.5, booster='gbtree'...    0.905028  0.806993   \n",
       "8   XGBClassifier(base_score=0.5, booster='gbtree'...    0.905028  0.871508   \n",
       "4         {'n_estimators': 273, 'learning_rate': 1.6}    0.899441  0.799089   \n",
       "3   {'max_depth': 10, 'n_estimators': 445, 'max_le...    0.882682  0.813165   \n",
       "13  {'svc__gamma': 0.00278255940221, 'pca__n_compo...    0.865922  0.804684   \n",
       "0            {'weights': 'uniform', 'n_neighbors': 5}    0.860335  0.810308   \n",
       "12   {'svc__gamma': 0.00278255940221, 'svc__C': 3053}    0.860335  0.804703   \n",
       "9   LogisticRegression(C=1.0, class_weight=None, d...    0.815642  0.801117   \n",
       "11  LinearSVC(C=1.0, class_weight=None, dual=True,...    0.810056  0.785475   \n",
       "1                                     {'loss': 'log'}    0.804469  0.766833   \n",
       "10  {'max_iter': 27, 'hidden_layer_sizes': 215, 'e...    0.687151  0.647645   \n",
       "\n",
       "    CV STDEV  \n",
       "5   0.034340  \n",
       "2   0.025687  \n",
       "6   0.023807  \n",
       "7   0.021936  \n",
       "8   0.000000  \n",
       "4   0.024731  \n",
       "3   0.014864  \n",
       "13  0.021931  \n",
       "0   0.020743  \n",
       "12  0.017722  \n",
       "9   0.013955  \n",
       "11  0.019223  \n",
       "1   0.033926  \n",
       "10  0.033133  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = results.sort_values(by=[\"CV Mean\"], ascending=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Voting: Model Ensemble\n",
    "<a id=\"Vote\"></a>\n",
    "Like the system that enabled multiple trees to predict together, this system brings together models of all types. A broader implementation.\n",
    "\n",
    "- **Hard Voting:** Plurality voting over the classes.\n",
    "- **Soft Voting:** Selects classed based off aggregated probabilities over the models. Requires model with probabilistic capabilities, which is why I removed linear SCV from inclusion. This difference is tremendous, while hard votes may polarize a small difference between models, say the prediction of 51% Survived and 49% Dead, the soft voting is able to make a more nuanced decision, rewarding high confidence models, and penalizing low confidence.\n",
    "- **Weights:** This is an additional feature which enables manual assigning voting power to each voting model.\n",
    "\n",
    "## Prepare and Observe Voting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Ensemble Voting\n",
    "not_proba_list = ('LinearSV','StochasticGradientDescent')\n",
    "not_proba =  results.query(\"Model in @not_proba_list\")\n",
    "hard_models = results # All Can Be\n",
    "prob_models = results.query(\"Model not in @not_proba_list\") # Not All\n",
    "# [x for x in results.Model if x not in not_proba_list]\n",
    "\n",
    "# Submission DataFrame for correlation purposes\n",
    "# Hard Output\n",
    "test_hard_pred_matrix = pd.DataFrame()\n",
    "train_hard_pred_matrix = pd.DataFrame()\n",
    "\n",
    "#Soft Output\n",
    "test_soft_pred_matrix = pd.DataFrame()\n",
    "train_soft_pred_matrix = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.80 (+/- 0.02) [AdaBoost_Ensemble]\n",
      "Test Accuracy: 0.90 \n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [Bagger_ensemble]\n",
      "Test Accuracy: 0.94 \n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [Random_Forest]\n",
      "Test Accuracy: 0.88 \n",
      "Train CV Accuracy: 0.79 (+/- 0.03) [XGBsklearn]\n",
      "Test Accuracy: 0.91 \n",
      "Train CV Accuracy: 0.80 (+/- 0.02) [SVCrbf]\n",
      "Test Accuracy: 0.86 \n",
      "Train CV Accuracy: 0.79 (+/- 0.03) [XGBstandard]\n",
      "Test Accuracy: 0.91 \n",
      "Train CV Accuracy: 0.80 (+/- 0.02) [PCA_SVC]\n",
      "Test Accuracy: 0.87 \n",
      "Train CV Accuracy: 0.64 (+/- 0.06) [RSNeural_Net]\n",
      "Test Accuracy: 0.69 \n",
      "Train CV Accuracy: 0.79 (+/- 0.03) [LinearSV]\n",
      "Test Accuracy: 0.81 \n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [KNN]\n",
      "Test Accuracy: 0.86 \n",
      "Train CV Accuracy: 0.71 (+/- 0.12) [StochasticGradientDescent]\n",
      "Test Accuracy: 0.80 \n",
      "Train CV Accuracy: 0.79 (+/- 0.03) [Logistic_Regression]\n",
      "Test Accuracy: 0.82 \n",
      "Train CV Accuracy: 0.81 (+/- 0.04) [Gradient_Boosting]\n",
      "Test Accuracy: 0.95 \n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [Sci_kit XGB]\n",
      "Test Accuracy: 0.93 \n"
     ]
    }
   ],
   "source": [
    "# None Probabilistic\n",
    "models = list(zip([ensemble_models[x] for x in not_proba.Model],\n",
    "                  not_proba.Model))\n",
    "clfs = []\n",
    "print('5-fold cross validation:\\n')\n",
    "for clf, label in models:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring=scoring, verbose=0)\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X_train, y_train)    \n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \\n\" % (metrics.accuracy_score(clf.predict(X_test), y_test)))\n",
    "    \n",
    "    # Model on Full Data\n",
    "    md = clf.fit(X,y)\n",
    "    submission = md.predict(test_df)\n",
    "    df = pd.DataFrame({'PassengerId':test_df.index, \n",
    "                           'Survived':submission})\n",
    "    train_hard_pred_matrix = pd.concat((train_hard_pred_matrix, pd.DataFrame({label: md.predict(X)})), axis=1)\n",
    "    test_hard_pred_matrix = pd.concat((test_hard_pred_matrix, pd.DataFrame({label: submission})), axis=1)\n",
    "    df.to_csv(\"{}.csv\".format(label),header=True,index=False)\n",
    "\n",
    "del clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only Probabilistic\n",
    "models = list(zip([ensemble_models[x] for x in prob_models.Model],\n",
    "                  prob_models.Model))\n",
    "plt.figure()\n",
    "\n",
    "print('5-fold cross validation:\\n')\n",
    "clfs = []\n",
    "#[ensemble_models[x] for x in prob_models.Model]\n",
    "for clf, label in models:\n",
    "    scores = cross_val_score(clf, X_train, y_train,cv=5, scoring=scoring, verbose=0)\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X_train, y_train)\n",
    "    # Add to Roc Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, md.predict_proba(X_test)[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    print('ROC AUC: %0.2f' % roc_auc)\n",
    "    plt.plot(fpr, tpr, label='{} ROC curve (area = {:.2})'.format(label, roc_auc))\n",
    "    \n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \\n\" % (metrics.accuracy_score(clf.predict(X_test), y_test)))\n",
    "    \n",
    "    # Model on Full Data\n",
    "    md = clf.fit(X,y)\n",
    "    submission = md.predict(test_df)\n",
    "    df = pd.DataFrame({'PassengerId':test_df.index, \n",
    "                           'Survived':submission})\n",
    "    train_hard_pred_matrix = pd.concat((train_hard_pred_matrix, pd.DataFrame({label: md.predict(X)})), axis=1)\n",
    "    test_hard_pred_matrix = pd.concat((test_hard_pred_matrix, pd.DataFrame({label: submission})), axis=1)\n",
    "\n",
    "    train_soft_pred_matrix = pd.concat((train_soft_pred_matrix, pd.DataFrame({label: md.predict(X)})), axis=1)\n",
    "    test_soft_pred_matrix = pd.concat((test_soft_pred_matrix, pd.DataFrame({label: md.predict_proba(test_df)[:,1]})), axis=1)\n",
    "    df.to_csv(\"{}.csv\".format(label),header=True,index=False)\n",
    "\n",
    "# Plot\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction to Receiver Operating Characteristic curve [ROC]\n",
    "ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. Well said wikipedia. Closer the line is to the top left, the better its predictive ability. Non-Smooth curve suggest important thresholds, effective a cluster of probabilities near a swing point.\n",
    "\n",
    "\n",
    "# Soft and Hard Voters of Difference Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2-Voting Models: 5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.81 (+/- 0.03) [2-VM-Ensemble Soft Voting]\n",
      "Test Accuracy: 0.94 \n",
      "Train CV Accuracy: 0.83 (+/- 0.03) [2-VM-Ensemble Hard Voting]\n",
      "Test Accuracy: 0.94 \n",
      "\n",
      "\n",
      "3-Voting Models: 5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.82 (+/- 0.03) [3-VM-Ensemble Soft Voting]\n",
      "Test Accuracy: 0.94 \n",
      "Train CV Accuracy: 0.82 (+/- 0.03) [3-VM-Ensemble Hard Voting]\n",
      "Test Accuracy: 0.95 \n",
      "\n",
      "\n",
      "5-Voting Models: 5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [5-VM-Ensemble Soft Voting]\n",
      "Test Accuracy: 0.95 \n",
      "Train CV Accuracy: 0.81 (+/- 0.03) [5-VM-Ensemble Hard Voting]\n",
      "Test Accuracy: 0.94 \n",
      "\n",
      "\n",
      "7-Voting Models: 5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.81 (+/- 0.03) [7-VM-Ensemble Soft Voting]\n",
      "Test Accuracy: 0.94 \n",
      "Train CV Accuracy: 0.80 (+/- 0.03) [7-VM-Ensemble Hard Voting]\n",
      "Test Accuracy: 0.93 \n",
      "\n",
      "\n",
      "10-Voting Models: 5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.82 (+/- 0.03) [10-VM-Ensemble Soft Voting]\n",
      "Test Accuracy: 0.91 \n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [10-VM-Ensemble Hard Voting]\n",
      "Test Accuracy: 0.91 \n"
     ]
    }
   ],
   "source": [
    "# Play with Weights\n",
    "plt.figure()\n",
    "voters = {}\n",
    "for x in [2,3,5,7,10]:\n",
    "    ECH = EnsembleVoteClassifier([ensemble_models.get(key) for key in hard_models.Model[:x]], voting='hard')\n",
    "    ECS = EnsembleVoteClassifier([ensemble_models.get(key) for key in prob_models.Model[:x]], voting='soft')\n",
    "    print('\\n{}-Voting Models: 5-fold cross validation:\\n'.format(x))\n",
    "    \n",
    "    for clf, label in zip([ECS, ECH], \n",
    "                          ['{}-VM-Ensemble Soft Voting'.format(x),\n",
    "                           '{}-VM-Ensemble Hard Voting'.format(x)]):\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "        md = clf.fit(X_train, y_train)    \n",
    "        clfs.append(md)        \n",
    "        \n",
    "        Test_Score = metrics.accuracy_score(clf.predict(X_test), y_test)\n",
    "        print(\"Test Accuracy: %0.2f \" % Test_Score)\n",
    "        \n",
    "        CV_Score = scores.mean()\n",
    "        STDev = scores.std()\n",
    "        \n",
    "        global results\n",
    "        results = results.append({'Model': label,'Para': clf, 'CV Mean': CV_Score,\n",
    "                'Test_Score':Test_Score,'CV STDEV': STDev}, ignore_index=True)\n",
    "        voters[label] = clf\n",
    "        \n",
    "        # Model on Full Data\n",
    "        md = clf.fit(X,y)\n",
    "        submission = md.predict(test_df)\n",
    "        df = pd.DataFrame({'PassengerId':test_df.index,'Survived':submission})\n",
    "        df.to_csv(\"{}.csv\".format(label),header=True,index=False)\n",
    "        \n",
    "        if clf is ECH:\n",
    "            # Hard Correlation\n",
    "            train_hard_pred_matrix = pd.concat((train_hard_pred_matrix, pd.DataFrame({label: md.predict(X)})), axis=1)\n",
    "            test_hard_pred_matrix = pd.concat((test_hard_pred_matrix, pd.DataFrame({label: submission})), axis=1)\n",
    "        \n",
    "        if clf is ECS:\n",
    "            # Add to Roc Curve\n",
    "            fpr, tpr, _ = roc_curve(y_test, md.predict_proba(X_test)[:,1])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            print('ROC AUC: %0.2f' % roc_auc)\n",
    "            plt.plot(fpr, tpr, label='{} ROC curve (area = {:.2})'.format(label, roc_auc))\n",
    "            # Soft Correlation\n",
    "            train_soft_pred_matrix = pd.concat((train_soft_pred_matrix, pd.DataFrame({label: md.predict(X)})), axis=1)\n",
    "            test_soft_pred_matrix = pd.concat((test_soft_pred_matrix, pd.DataFrame({label: md.predict_proba(test_df)[:,1]})), axis=1)\n",
    "        \n",
    "        \n",
    "# Plot\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Soft Model ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Voter Pipeline\n",
    "\n",
    "Talking about pipelines, Sklearn's ensemble voting is structured as one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voters.get('10-VM-Ensemble Hard Voting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Models\n",
    "<a id=\"Stack\"></a>\n",
    "“Stacking is a way of combining multiple models, that introduces the concept of a meta learner. It is less widely used than bagging and boosting. Unlike bagging and boosting, stacking may be (and normally is) used to combine models of different types.” - Anshul Joshi [1]\n",
    "\n",
    "Big shoutout to Manohar Swamynathan, author of Mastering Machine Learning with Python in Six Steps, who eloquently explains and took me step by step through stacks in Python. [2]\n",
    "\n",
    "Next:\n",
    "Could target high false negative through stacking methods, since it builds models off model weakness.\n",
    "\n",
    "Read more here:\n",
    "\n",
    "[1] https://www.quora.com/What-is-stacking-in-machine-learning \n",
    "\n",
    "[2] https://github.com/Apress/mastering-ml-w-python-in-six-steps/blob/master/Chapter_4_Code/Code/Stacking.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xstack = X.copy()\n",
    "ystack = y.copy()\n",
    "X_trainstack = X_train.copy()\n",
    "X_teststack = X_test.copy()\n",
    "y_trainstack = y_train.copy()\n",
    "y_teststack = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 8) (891,) (418, 8)\n",
      "5-fold cross validation:\n",
      "\n",
      "##### Base Model 0 #####\n",
      "Train CV Accuracy: 0.80 (+/- 0.02)\n",
      "Train Accuracy: 0.85 \n",
      "Test Accuracy: 0.87 \n",
      "##### Base Model 1 #####\n",
      "Train CV Accuracy: 0.81 (+/- 0.02)\n",
      "Train Accuracy: 0.92 \n",
      "Test Accuracy: 0.84 \n",
      "##### Base Model 2 #####\n",
      "Train CV Accuracy: 0.82 (+/- 0.02)\n",
      "Train Accuracy: 0.83 \n",
      "Test Accuracy: 0.85 \n",
      "##### Meta Model #####\n",
      "Train CV Accuracy: 0.92 (+/- 0.01)\n",
      "Train Accuracy: 0.92 \n",
      "Test Accuracy: 0.83 \n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "kfold = cross_validation.StratifiedKFold(y=y_trainstack, n_folds=5, random_state=rstate)\n",
    "num_trees = 10\n",
    "verbose = True # to print the progress\n",
    "\n",
    "clfs = [ensemble_models.get('KNN'),\n",
    "        ensemble_models.get('XGBstandard')]\n",
    "\n",
    "# Creating train and test sets for blending\n",
    "dataset_blend_train = np.zeros((X_trainstack.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_teststack.shape[0], len(clfs)))\n",
    "dataset_blend_test_df = np.zeros((test_df.shape[0], len(clfs)))\n",
    "\n",
    "print('5-fold cross validation:')\n",
    "for i, clf in enumerate(clfs):   \n",
    "    scores = cross_validation.cross_val_score(clf, X_trainstack, y_trainstack, cv=kfold, scoring='accuracy')\n",
    "    print(\"##### Base Model %0.0f #####\" % i)\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "    clf.fit(X_trainstack, y_trainstack)   \n",
    "    print(\"Train Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_trainstack), y_trainstack)))\n",
    "    dataset_blend_train[:,i] = clf.predict_proba(X_trainstack)[:, 1]\n",
    "    dataset_blend_test[:,i] = clf.predict_proba(X_teststack)[:, 1]\n",
    "    dataset_blend_test_df[:,i] = clf.predict_proba(test_df)[:, 1]\n",
    "    print(\"Test Accuracy: %0.2f \\n\" % (metrics.accuracy_score(clf.predict(X_teststack), y_teststack)))    \n",
    "\n",
    "print(\"##### Meta Model #####\")\n",
    "clf = LogisticRegression()\n",
    "scores = cross_validation.cross_val_score(clf, dataset_blend_train, y_trainstack, cv=kfold, scoring=scoring)\n",
    "clf.fit(dataset_blend_train, y_trainstack)\n",
    "print(\"Train CV Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "print(\"Train Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(dataset_blend_train), y_trainstack)))\n",
    "print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(dataset_blend_test), y_teststack)))\n",
    "\n",
    "# Correlate Results\n",
    "#test_hard_pred_matrix = pd.concat((test_hard_pred_matrix, pd.DataFrame({label: clf.predict(dataset_blend_test_df)})), axis=1)\n",
    "#train_hard_pred_matrix = pd.concat((train_hard_pred_matrix, pd.DataFrame({label: model.predict(dataset_blend_train)})), axis=1)\n",
    "\n",
    "# Save\n",
    "pd.DataFrame({'PassengerId':test_df.index, \n",
    "    'Survived':clf.predict(dataset_blend_test_df)}).to_csv(\n",
    "    \"{}.csv\".format(\"Stacked\"),header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.81 (+/- 0.01) [stacked]\n",
      "Test_Score: 0.821229050279\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(clf, X, y, cv=cv, scoring=scoring)\n",
    "norm_save(clf, score, \"stacked\")\n",
    "eval_plot(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Results\n",
    "<a id=\"TOR\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Para</th>\n",
       "      <th>Test_Score</th>\n",
       "      <th>CV Mean</th>\n",
       "      <th>CV STDEV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient_Boosting</td>\n",
       "      <td>{'loss': 'deviance', 'max_depth': 2.5, 'n_esti...</td>\n",
       "      <td>0.949721</td>\n",
       "      <td>0.815922</td>\n",
       "      <td>0.034340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[GradientBoostingC...</td>\n",
       "      <td>0.949721</td>\n",
       "      <td>0.808879</td>\n",
       "      <td>0.021658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[GradientBoostingC...</td>\n",
       "      <td>0.949721</td>\n",
       "      <td>0.815882</td>\n",
       "      <td>0.028911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagger_ensemble</td>\n",
       "      <td>{'n_estimators': 236}</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.808850</td>\n",
       "      <td>0.025687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[GradientBoostingC...</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.811677</td>\n",
       "      <td>0.027079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[GradientBoostingC...</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.818660</td>\n",
       "      <td>0.033568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[GradientBoostingC...</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.825751</td>\n",
       "      <td>0.029987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[GradientBoostingC...</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.813075</td>\n",
       "      <td>0.028256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[GradientBoostingC...</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.813055</td>\n",
       "      <td>0.031093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[GradientBoostingC...</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.806062</td>\n",
       "      <td>0.028580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sci_kit XGB</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 379, 'subsamp...</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.814504</td>\n",
       "      <td>0.023807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[GradientBoostingC...</td>\n",
       "      <td>0.927374</td>\n",
       "      <td>0.803245</td>\n",
       "      <td>0.026611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10-VM-Ensemble Soft Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[GradientBoostingC...</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.817261</td>\n",
       "      <td>0.029593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBstandard</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.871508</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBsklearn</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.806993</td>\n",
       "      <td>0.021936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10-VM-Ensemble Hard Voting</td>\n",
       "      <td>EnsembleVoteClassifier(clfs=[GradientBoostingC...</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.814523</td>\n",
       "      <td>0.017385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoost_Ensemble</td>\n",
       "      <td>{'n_estimators': 273, 'learning_rate': 1.6}</td>\n",
       "      <td>0.899441</td>\n",
       "      <td>0.799089</td>\n",
       "      <td>0.024731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 445, 'max_le...</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.813165</td>\n",
       "      <td>0.014864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PCA_SVC</td>\n",
       "      <td>{'svc__gamma': 0.00278255940221, 'pca__n_compo...</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.804684</td>\n",
       "      <td>0.021931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SVCrbf</td>\n",
       "      <td>{'svc__gamma': 0.00278255940221, 'svc__C': 3053}</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.804703</td>\n",
       "      <td>0.017722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>{'weights': 'uniform', 'n_neighbors': 5}</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.810308</td>\n",
       "      <td>0.020743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>stacked</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.821229</td>\n",
       "      <td>0.806704</td>\n",
       "      <td>0.011503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.801117</td>\n",
       "      <td>0.013955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LinearSV</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.785475</td>\n",
       "      <td>0.019223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>StochasticGradientDescent</td>\n",
       "      <td>{'loss': 'log'}</td>\n",
       "      <td>0.804469</td>\n",
       "      <td>0.766833</td>\n",
       "      <td>0.033926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RSNeural_Net</td>\n",
       "      <td>{'max_iter': 27, 'hidden_layer_sizes': 215, 'e...</td>\n",
       "      <td>0.687151</td>\n",
       "      <td>0.647645</td>\n",
       "      <td>0.033133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  \\\n",
       "0            Gradient_Boosting   \n",
       "19   5-VM-Ensemble Soft Voting   \n",
       "18   3-VM-Ensemble Hard Voting   \n",
       "1              Bagger_ensemble   \n",
       "21   7-VM-Ensemble Soft Voting   \n",
       "17   3-VM-Ensemble Soft Voting   \n",
       "16   2-VM-Ensemble Hard Voting   \n",
       "15   2-VM-Ensemble Soft Voting   \n",
       "14   2-VM-Ensemble Soft Voting   \n",
       "20   5-VM-Ensemble Hard Voting   \n",
       "2                  Sci_kit XGB   \n",
       "22   7-VM-Ensemble Hard Voting   \n",
       "23  10-VM-Ensemble Soft Voting   \n",
       "4                  XGBstandard   \n",
       "3                   XGBsklearn   \n",
       "24  10-VM-Ensemble Hard Voting   \n",
       "5            AdaBoost_Ensemble   \n",
       "6                Random_Forest   \n",
       "7                      PCA_SVC   \n",
       "9                       SVCrbf   \n",
       "8                          KNN   \n",
       "25                     stacked   \n",
       "10         Logistic_Regression   \n",
       "11                    LinearSV   \n",
       "12   StochasticGradientDescent   \n",
       "13                RSNeural_Net   \n",
       "\n",
       "                                                 Para  Test_Score   CV Mean  \\\n",
       "0   {'loss': 'deviance', 'max_depth': 2.5, 'n_esti...    0.949721  0.815922   \n",
       "19  EnsembleVoteClassifier(clfs=[GradientBoostingC...    0.949721  0.808879   \n",
       "18  EnsembleVoteClassifier(clfs=[GradientBoostingC...    0.949721  0.815882   \n",
       "1                               {'n_estimators': 236}    0.944134  0.808850   \n",
       "21  EnsembleVoteClassifier(clfs=[GradientBoostingC...    0.938547  0.811677   \n",
       "17  EnsembleVoteClassifier(clfs=[GradientBoostingC...    0.938547  0.818660   \n",
       "16  EnsembleVoteClassifier(clfs=[GradientBoostingC...    0.938547  0.825751   \n",
       "15  EnsembleVoteClassifier(clfs=[GradientBoostingC...    0.938547  0.813075   \n",
       "14  EnsembleVoteClassifier(clfs=[GradientBoostingC...    0.938547  0.813055   \n",
       "20  EnsembleVoteClassifier(clfs=[GradientBoostingC...    0.938547  0.806062   \n",
       "2   {'max_depth': 7, 'n_estimators': 379, 'subsamp...    0.932961  0.814504   \n",
       "22  EnsembleVoteClassifier(clfs=[GradientBoostingC...    0.927374  0.803245   \n",
       "23  EnsembleVoteClassifier(clfs=[GradientBoostingC...    0.905028  0.817261   \n",
       "4   XGBClassifier(base_score=0.5, booster='gbtree'...    0.905028  0.871508   \n",
       "3   XGBClassifier(base_score=0.5, booster='gbtree'...    0.905028  0.806993   \n",
       "24  EnsembleVoteClassifier(clfs=[GradientBoostingC...    0.905028  0.814523   \n",
       "5         {'n_estimators': 273, 'learning_rate': 1.6}    0.899441  0.799089   \n",
       "6   {'max_depth': 10, 'n_estimators': 445, 'max_le...    0.882682  0.813165   \n",
       "7   {'svc__gamma': 0.00278255940221, 'pca__n_compo...    0.865922  0.804684   \n",
       "9    {'svc__gamma': 0.00278255940221, 'svc__C': 3053}    0.860335  0.804703   \n",
       "8            {'weights': 'uniform', 'n_neighbors': 5}    0.860335  0.810308   \n",
       "25  LogisticRegression(C=1.0, class_weight=None, d...    0.821229  0.806704   \n",
       "10  LogisticRegression(C=1.0, class_weight=None, d...    0.815642  0.801117   \n",
       "11  LinearSVC(C=1.0, class_weight=None, dual=True,...    0.810056  0.785475   \n",
       "12                                    {'loss': 'log'}    0.804469  0.766833   \n",
       "13  {'max_iter': 27, 'hidden_layer_sizes': 215, 'e...    0.687151  0.647645   \n",
       "\n",
       "    CV STDEV  \n",
       "0   0.034340  \n",
       "19  0.021658  \n",
       "18  0.028911  \n",
       "1   0.025687  \n",
       "21  0.027079  \n",
       "17  0.033568  \n",
       "16  0.029987  \n",
       "15  0.028256  \n",
       "14  0.031093  \n",
       "20  0.028580  \n",
       "2   0.023807  \n",
       "22  0.026611  \n",
       "23  0.029593  \n",
       "4   0.000000  \n",
       "3   0.021936  \n",
       "24  0.017385  \n",
       "5   0.024731  \n",
       "6   0.014864  \n",
       "7   0.021931  \n",
       "9   0.017722  \n",
       "8   0.020743  \n",
       "25  0.011503  \n",
       "10  0.013955  \n",
       "11  0.019223  \n",
       "12  0.033926  \n",
       "13  0.033133  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by=[\"CV Mean\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Correlate the Results:\n",
    "<a id=\"COR\"></a>\n",
    "\n",
    "What does it mean to correlate these values?\n",
    "\n",
    "What if I used a \"match-up\" (\"Accuracy\" between outputs) rate instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samplesize = 5\n",
    "\n",
    "# Train Prep\n",
    "train_hard_pred_matrix = train_hard_pred_matrix.set_index([traindex])\n",
    "train_hard_pred_matrix = pd.concat([train_hard_pred_matrix, Survived], axis=1)\n",
    "# Soft\n",
    "train_soft_pred_matrix = train_soft_pred_matrix.set_index([traindex])\n",
    "train_soft_pred_matrix = pd.concat([train_soft_pred_matrix, Survived], axis=1)\n",
    "\n",
    "# Test Prep\n",
    "test_hard_pred_matrix = test_hard_pred_matrix.set_index([testdex])\n",
    "# Soft\n",
    "test_soft_pred_matrix = test_soft_pred_matrix.set_index([testdex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# View Example\n",
    "train_hard_pred_matrix.sample(samplesize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hard Correlation, All Models\n",
    "sns.heatmap(train_hard_pred_matrix.corr(), annot=True, fmt=\".2f\",cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_soft_pred_matrix.sample(samplesize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hard Correlation, All Models\n",
    "sns.heatmap(train_soft_pred_matrix.corr(), annot=True, fmt=\".2f\",cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dont think Correlation does this justice. Will try to have \"match-up\" rates between output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_hard_pred_matrix.sample(samplesize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hard Correlation, All Models\n",
    "sns.heatmap(test_hard_pred_matrix.corr(), annot=True, fmt=\".2f\",cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_soft_pred_matrix.sample(samplesize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hard Correlation, All Models\n",
    "sns.heatmap(test_soft_pred_matrix.corr(), annot=True, fmt=\".2f\",cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Pobabilistic Output: [Explore Stacking](https://www.kaggle.com/dongxu027/explore-stacking-lb-0-1463)\n",
    "\n",
    "1. **Mean Stacking**\n",
    "2. **Median Stacking**\n",
    "3. **PushOut + Median Stacking**\n",
    "4. **MinMax + Mean Stacking**\n",
    "5. **MinMax + Median Stacking**\n",
    "6. **MinMax + BestBase Stacking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up cutoff threshold for lower and upper bounds, easy to twist \n",
    "cutoff_lo = 0.8\n",
    "cutoff_hi = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trial = train_soft_pred_matrix.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_sub['is_iceberg_max'] = concat_sub.iloc[:, 1:6].max(axis=1)\n",
    "concat_sub['is_iceberg_min'] = concat_sub.iloc[:, 1:6].min(axis=1)\n",
    "concat_sub['is_iceberg_mean'] = concat_sub.iloc[:, 1:6].mean(axis=1)\n",
    "concat_sub['is_iceberg_median'] = concat_sub.iloc[:, 1:6].median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_sub['is_iceberg'] = concat_sub['is_iceberg_mean']\n",
    "concat_sub[['id', 'is_iceberg']].to_csv('stack_mean.csv', \n",
    "                                        index=False, float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Evaluation\n",
    "[Scikit-Plot Documentation](http://scikit-plot.readthedocs.io/en/stable/Quickstart.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.89      0.87       119\n",
      "          1       0.76      0.68      0.72        60\n",
      "\n",
      "avg / total       0.82      0.82      0.82       179\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[106,  13],\n",
       "       [ 19,  41]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reinstate Data, since it was meddled with during Stacked Models\n",
    "X = train_df.drop([\"Survived\"] , axis=1)\n",
    "y = train_df[\"Survived\"]\n",
    "\n",
    "# Stratified Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "\n",
    "evalmodel = EnsembleVoteClassifier([ensemble_models.get(key) for key in prob_models.Model[:7]], voting='soft')\n",
    "evalmodel.fit(X_train, y_train)\n",
    "y_pred = evalmodel.predict(X_test)\n",
    "# Report\n",
    "print(\"\\n Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "confusion_matrix(y_test, y_pred)\n",
    "# Matrix\n",
    "print(\"\\n Matrix:\")\n",
    "skplt.metrics.plot_confusion_matrix(y_pred, y_test, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predict Submission Set and Output to CSV\n",
    "# clf = EnsembleVoteClassifier([ensemble_models.get(key) for key in prob_models.Model[:7]], voting='soft')\n",
    "# md = clf.fit(X, y)\n",
    "# df = pd.DataFrame({'PassengerId':test_df.index, 'Survived':md.predict(test_df)})\n",
    "# df.to_csv(\"Soft_Voting_7_TopModel\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model took 2844.75 seconds to train\n"
     ]
    }
   ],
   "source": [
    "# Save Results\n",
    "results.to_csv(\"Results.csv\", index_label=False)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Notebook took %0.2f minutes to Run\"%((end - start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reflection\n",
    "<a id=\"REFL\"></a>\n",
    "A recurrent theme that I have observed is that the accuracy on the testing set is always higher than that of the submission set. This suggests that there is a disconnected representation of the underlying data distributions. A likely contributor to this problem is the quality of pre-processing, whose features appeared to have redundant effects, such as Title and Sex. A quick fix could be dimensionality reduction, but ultimately if proper exploration of features is conducted, then the “garbage in” problem can be minimized.\n",
    "\n",
    "I think for this type of problem, elaborating on the stacked methodology might be key.\n",
    "\n",
    "### On Confusion Matrix\n",
    "Accross the board, most errors are false negatives. The model expected many of the deceased to have survived. This is perhaps why the ensembling only provided minor improvements, since it combined models with the same underlying prediction problem. \n",
    "\n",
    "Note: Deep Learning references are to be taken **LIGHTLY!**, currently reading Ian Goodfellow’s book, so this stuff is on my mind.\n",
    "\n",
    "### Thank You for making it this far! Next, I am thinking of either pursuing a wide application Regression models on another dataset, or doubling down on the exploratory analysis and feature engineering for Titanic! \n",
    "\n",
    "### *What say you?* P.S, Looking for constructive feedback :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
